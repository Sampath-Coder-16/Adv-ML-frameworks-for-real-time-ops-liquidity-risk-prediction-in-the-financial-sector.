# -*- coding: utf-8 -*-
"""Bank Liquidity Risk Prediction: Comparative Model Performance Report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xP6vAlp5wwsgtmHONHgdUhPwX7isX3N4

Bank Liquidity Risk Prediction: Comparative Model Performance Report

Project Aim

The aim of this project is to conduct a comprehensive comparative analysis of various machine learning algorithms to identify the most effective and suitable model for predicting bank liquidity risk. Building upon the feature engineering and data preparation methodologies of the foundational research paper by Barongo & Mbelwa (2024), this analysis seeks to benchmark the performance of standard classifiers—including Logistic Regression, Decision Trees, K-Nearest Neighbors (KNN), and XGBoost—against the specialized hybrid model to provide a clear, data-driven recommendation for implementing a robust, high-accuracy early warning system.

1.EXPLORATORY DATA ANALYSIS (EDA)

1.1 Importing Libaries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

"""1.2 Loading of dataset"""

df = pd.read_csv('/content/BankLiquidityRiskDetection.csv')

df

df.dtypes

"""1.3 Checking the Null Values"""

df.isnull().sum()

"""1. Droping The null Values"""

df.drop_duplicates(inplace=True)
print("Duplicate rows dropped.")
display(df.head())

"""Droped duplicated rows

1.5 Droping the unwanted coloumn
"""

df = df.drop(columns=['Unnamed: 0', 'REPORTINGDATE', 'INSTITUTIONCODE'])
display(df.head())

missing_values = df.isnull().sum()
display(missing_values)

"""1.6 Checking Outcomes coloumns and target varabile"""

df['EWL_LIQUIDITY RATING'].value_counts()
df['MLA_CLASS2'].value_counts()
df['XX_MLA_CLASS2'].value_counts() # Trget variable

"""1.7 Numericals coloumns and categorical coloumns"""

numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
categorical_cols = df.select_dtypes(include='object').columns.tolist()

print("Numerical Columns:")
display(numerical_cols)

print("\nCategorical Columns:")
display(categorical_cols)

"""1.8 Histograms(DATA VISULATION)"""

df.hist(figsize=(20, 20))
plt.tight_layout()
plt.show()

"""1.8 Checing the relation with Trget variable (graph represnation)"""

# Select the target variable and numerical columns
target = 'XX_MLA_CLASS2'
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

# Remove target if it exists in numerical columns
if target in numerical_cols:
    numerical_cols.remove(target)

# Plot box plots for each numerical column against the target variable
plt.figure(figsize=(20, 5 * ((len(numerical_cols) - 1) // 4 + 1)))  # Dynamic height

for i, col in enumerate(numerical_cols):
    plt.subplot((len(numerical_cols) - 1) // 4 + 1, 4, i + 1)
    sns.boxplot(x=target, y=col, data=df)
    plt.title(f'{col} vs {target}')

plt.tight_layout()
plt.show()

""" 2.Experimenting with the dataset using machine learning algorithms (Random Forest and MLP) on imbalanced raw data."""

# 1. Randomforest
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

#split and Scale Imbalanced Data ---
# Using the original 'features_df' and 'y' before balancing
X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(
    features_df, y, test_size=0.33, random_state=42, stratify=y # stratify is good practice for imbalanced data
)
scaler_imb = StandardScaler()
X_train_imb_scaled = scaler_imb.fit_transform(X_train_imb)
X_test_imb_scaled = scaler_imb.transform(X_test_imb)
print("Imbalanced data split and scaled.")

#Replicating with 8 of the 10 features available in the dataset.
features_df = pd.DataFrame()
features_df['X12_BankSize'] = np.log10(df['F077_ASSETS_TOTAL'].replace(0, 1))
features_df['X14_GDP_Growth'] = df['GDP']
features_df['X15_Loan_to_Assets'] = df['EWAQ_GrossLoans'].div(df['F077_ASSETS_TOTAL']).fillna(0)
features_df['X16_NPL_Ratio'] = df['EWAQ_NPL'].div(df['EWAQ_GrossLoans']).fillna(0)
features_df['X17_NPL_Net_Prov_Ratio'] = df['EWAQ_NPLsNetOfProvisions'].div(df['EWAQ_GrossLoans']).fillna(0)
features_df['X18_NPLNP_to_CoreCapital'] = df['EWAQ_NPLsNetOfProvisions2CoreCapital']
features_df['X20_Lending_Rate'] = df['LR']
features_df['X22_LATA'] = df['XX_TOTAL_LIQUID_ASSET'].div(df['F077_ASSETS_TOTAL']).fillna(0)
y = df['XX_MLA_CLASS2']
print("Feature engineering complete.")

# Random Forest Model
rf_model_imb = RandomForestClassifier(n_estimators=199, random_state=42)
rf_model_imb.fit(X_train_imb_scaled, y_train_imb)
rf_preds_imb = rf_model_imb.predict(X_test_imb_scaled)
rf_accuracy_imb = accuracy_score(y_test_imb, rf_preds_imb)
print(f"Random Forest Accuracy (Imbalanced): {rf_accuracy_imb:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score
from sklearn.preprocessing import label_binarize
import numpy as np
print("--- Random Forest Performance Measures (Imbalanced Data) ---")
# Calculate standard metrics
# Accuracy is already calculated in cell 3_E_sgGVQ77v as rf_accuracy_imb
balanced_accuracy_imb = balanced_accuracy_score(y_test_imb, rf_preds_imb)
precision_imb = precision_score(y_test_imb, rf_preds_imb, average='macro', zero_division=0)
recall_imb = recall_score(y_test_imb, rf_preds_imb, average='macro', zero_division=0)
f1_imb = f1_score(y_test_imb, rf_preds_imb, average='macro', zero_division=0)
kappa_imb = cohen_kappa_score(y_test_imb, rf_preds_imb)
# AUC requires probabilities
rf_probabilities_imb = rf_model_imb.predict_proba(X_test_imb_scaled)
# Binarize the true labels for multi-class AUC
ALL_CLASSES_IMB = sorted(y_test_imb.unique()) # Define classes based on imbalanced test set
y_test_imb_binarized = label_binarize(y_test_imb, classes=ALL_CLASSES_IMB)
auc_imb = roc_auc_score(y_test_imb_binarized, rf_probabilities_imb, multi_class='ovr', average='macro')
# Confusion Matrix
cm_imb = confusion_matrix(y_test_imb, rf_preds_imb, labels=ALL_CLASSES_IMB)
# Metrics calculated from the Confusion Matrix (Type I and Type II Error)
n_classes_imb = cm_imb.shape[0]
metrics_agg_imb = {'sensitivity': 0, 'specificity': 0}
for i in range(n_classes_imb):
    tp = cm_imb[i, i]
    fn = np.sum(cm_imb[i, :]) - tp
    # Fix: Use cm_imb instead of cm
    fp = np.sum(cm_imb[:, i]) - tp
    # Fix: Use cm_imb instead of cm
    tn = np.sum(cm_imb) - (tp + fp + fn)
    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_imb['sensitivity'] += sensitivity_class
    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_imb['specificity'] += specificity_class
sensitivity_macro_imb = metrics_agg_imb['sensitivity'] / n_classes_imb
specificity_macro_imb = metrics_agg_imb['specificity'] / n_classes_imb
type1_error_imb = 1 - specificity_macro_imb
type2_error_imb = 1 - sensitivity_macro_imb
g_mean_imb = np.sqrt(sensitivity_macro_imb * specificity_macro_imb)
youdens_index_imb = sensitivity_macro_imb + specificity_macro_imb - 1
nlir_imb = type2_error_imb / specificity_macro_imb if specificity_macro_imb > 0 else 0
discriminant_power_imb = np.log((sensitivity_macro_imb / type1_error_imb) * (specificity_macro_imb / type2_error_imb)) if type1_error_imb > 0 and type2_error_imb > 0 else 0
# Display key metrics as percentages
print("\nKey Performance Metrics (as Percentages):")
print(f"Accuracy: {rf_accuracy_imb * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_imb * 100:.2f}%")
print(f"Precision (Macro): {precision_imb * 100:.2f}%")
print(f"Recall (Macro): {recall_imb * 100:.2f}%")
print(f"F1 Score (Macro): {f1_imb * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_imb * 100:.2f}%")
print(f"G-mean: {g_mean_imb * 100:.2f}%")
print(f"Youden's Index: {youdens_index_imb * 100:.2f}%")

type1_error_imb = 1 - specificity_macro_imb
type2_error_imb = 1 - sensitivity_macro_imb
g_mean_imb = np.sqrt(sensitivity_macro_imb * specificity_macro_imb)
youdens_index_imb = sensitivity_macro_imb + specificity_macro_imb - 1
nlir_imb = type2_error_imb / specificity_macro_imb if specificity_macro_imb > 0 else 0
discriminant_power_imb = np.log((sensitivity_macro_imb / type1_error_imb) * (specificity_macro_imb / type2_error_imb)) if type1_error_imb > 0 and type2_error_imb > 0 else 0


print(f"\nType I Error (False Positive Rate): {type1_error_imb:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_imb:.4f}")
print(f"G-mean: {g_mean_imb:.4f}")
print(f"Youden's Index: {youdens_index_imb:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_imb:.4f}")
print(f"Discriminant Power: {discriminant_power_imb:.4f}")

import seaborn as sns
import matplotlib.pyplot as plt

# Metrics calculated from the Confusion Matrix (Type I and Type II Error)
n_classes_imb = cm_imb.shape[0]
metrics_agg_imb = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_imb):
    tp = cm_imb[i, i]
    fn = np.sum(cm_imb[i, :]) - tp
    fp = np.sum(cm_imb[:, i]) - tp
    tn = np.sum(cm_imb) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_imb['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_imb['specificity'] += specificity_class

sensitivity_macro_imb = metrics_agg_imb['sensitivity'] / n_classes_imb
specificity_macro_imb = metrics_agg_imb['specificity'] / n_classes_imb
print("--- Visualizing Random Forest Performance (Imbalanced Data) ---")
# Confusion Matrix Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_imb, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_IMB, yticklabels=ALL_CLASSES_IMB)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Random Forest (Imbalanced Data)')
plt.show()

# MLP Model
y_train_mlp_imb = y_train_imb - 1
y_test_mlp_imb = y_test_imb - 1
mlp_model_imb = Sequential([
    Dense(512, activation='relu', input_shape=(X_train_imb_scaled.shape[1],)),
    Dense(250, activation='relu'),
    Dense(120, activation='relu'),
    Dense(80, activation='relu'),
    Dense(60, activation='relu'),
    Dense(5, activation='softmax')
])
mlp_model_imb.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
mlp_model_imb.fit(X_train_imb_scaled, y_train_mlp_imb, epochs=30, verbose=0)
_, mlp_accuracy_imb = mlp_model_imb.evaluate(X_test_imb_scaled, y_test_mlp_imb, verbose=0)
print(f"MLP Accuracy (Imbalanced): {mlp_accuracy_imb:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming mlp_model_imb, X_test_imb_scaled, y_test_mlp_imb, mlp_accuracy_imb are available from previous cells
# Assuming y_test_imb is also available to get original class labels for confusion matrix and AUC

print("--- MLP Performance Measures (Imbalanced Data) ---")

# Get predictions and probabilities from the MLP model
mlp_probabilities_imb = mlp_model_imb.predict(X_test_imb_scaled)
mlp_predictions_raw_imb = np.argmax(mlp_probabilities_imb, axis=1)
# Convert MLP predictions back to original class labels (1-5)
mlp_preds_imb = mlp_predictions_raw_imb + 1

# Define classes based on the imbalanced test set for consistency
ALL_CLASSES_IMB = sorted(y_test_imb.unique())

# Calculate standard metrics
# Accuracy is available as mlp_accuracy_imb
balanced_accuracy_imb_mlp = balanced_accuracy_score(y_test_imb, mlp_preds_imb)
precision_imb_mlp = precision_score(y_test_imb, mlp_preds_imb, average='macro', zero_division=0)
recall_imb_mlp = recall_score(y_test_imb, mlp_preds_imb, average='macro', zero_division=0)
f1_imb_mlp = f1_score(y_test_imb, mlp_preds_imb, average='macro', zero_division=0)
kappa_imb_mlp = cohen_kappa_score(y_test_imb, mlp_preds_imb)

# AUC requires probabilities
y_test_imb_binarized = label_binarize(y_test_imb, classes=ALL_CLASSES_IMB)
auc_imb_mlp = roc_auc_score(y_test_imb_binarized, mlp_probabilities_imb, multi_class='ovr', average='macro')


# Display key metrics as percentages
print("\nKey Performance Metrics (as Percentages):")
print(f"Accuracy: {mlp_accuracy_imb * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_imb_mlp * 100:.2f}%")
print(f"Precision (Macro): {precision_imb_mlp * 100:.2f}%")
print(f"Recall (Macro): {recall_imb_mlp * 100:.2f}%")
print(f"F1 Score (Macro): {f1_imb_mlp * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_imb_mlp * 100:.2f}%")

# Confusion Matrix
cm_imb_mlp = confusion_matrix(y_test_imb, mlp_preds_imb, labels=ALL_CLASSES_IMB)
print("\nConfusion Matrix:")
display(cm_imb_mlp)

# Confusion Matrix Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_imb_mlp, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_IMB, yticklabels=ALL_CLASSES_IMB)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - MLP (Imbalanced Data)')
plt.show()

# Metrics calculated from the Confusion Matrix (Type I and Type II Error)
n_classes_imb_mlp = cm_imb_mlp.shape[0]
metrics_agg_imb_mlp = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_imb_mlp):
    tp = cm_imb_mlp[i, i]
    fn = np.sum(cm_imb_mlp[i, :]) - tp
    fp = np.sum(cm_imb_mlp[:, i]) - tp
    tn = np.sum(cm_imb_mlp) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_imb_mlp['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_imb_mlp['specificity'] += specificity_class

sensitivity_macro_imb_mlp = metrics_agg_imb_mlp['sensitivity'] / n_classes_imb_mlp
specificity_macro_imb_mlp = metrics_agg_imb_mlp['specificity'] / n_classes_imb_mlp

type1_error_imb_mlp = 1 - specificity_macro_imb_mlp
type2_error_imb_mlp = 1 - sensitivity_macro_imb_mlp
g_mean_imb_mlp = np.sqrt(sensitivity_macro_imb_mlp * specificity_macro_imb_mlp)
youdens_index_imb_mlp = sensitivity_macro_imb_mlp + specificity_macro_imb_mlp - 1
nlir_imb_mlp = type2_error_imb_mlp / specificity_macro_imb_mlp if specificity_macro_imb_mlp > 0 else 0
discriminant_power_imb_mlp = np.log((sensitivity_macro_imb_mlp / type1_error_imb_mlp) * (specificity_macro_imb_mlp / type2_error_imb_mlp)) if type1_error_imb_mlp > 0 and type2_error_imb_mlp > 0 else 0


print(f"\nType I Error (False Positive Rate): {type1_error_imb_mlp:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_imb_mlp:.4f}")
print(f"G-mean: {g_mean_imb_mlp:.4f}")
print(f"Youden's Index: {youdens_index_imb_mlp:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_imb_mlp:.4f}")
print(f"Discriminant Power: {discriminant_power_imb_mlp:.4f}")

"""1.9 Idenitication of Outliers and handling them

Identify, visualize, and handle outliers in the dataset.

Use a suitable method (e.g., Interquartile Range (IQR)) to identify outliers in the numerical features.
"""

numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

outlier_bounds = {}
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outlier_bounds[col] = {'Q1': Q1, 'Q3': Q3, 'IQR': IQR, 'lower_bound': lower_bound, 'upper_bound': upper_bound}

print("Outlier bounds (using IQR method) for each numerical column:")
for col, bounds in outlier_bounds.items():
    print(f"\n{col}:")
    print(f"  Q1: {bounds['Q1']:.2f}")
    print(f"  Q3: {bounds['Q3']:.2f}")
    print(f"  IQR: {bounds['IQR']:.2f}")
    print(f"  Lower Bound: {bounds['lower_bound']:.2f}")
    print(f"  Upper Bound: {bounds['upper_bound']:.2f}")

"""**Reasoning**:
Identify and count the number of outliers for each numerical column based on the calculated IQR bounds.


"""

outliers_count = {}
for col in numerical_cols:
    lower_bound = outlier_bounds[col]['lower_bound']
    upper_bound = outlier_bounds[col]['upper_bound']
    col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outliers_count[col] = col_outliers.shape[0]

print("\nNumber of outliers for each numerical column:")
for col, count in outliers_count.items():
    print(f"{col}: {count}")

"""Visualize outliers


Create visualizations (e.g., box plots) to show the identified outliers.

**Reasoning**:
Select a subset of columns with a significant number of outliers for visualization and create box plots to visualize the outliers.
"""

# Select columns with a significant number of outliers for visualization
cols_to_visualize = [col for col, count in outliers_count.items() if count > 0]

# Visualize outliers for a subset of columns
# Choose a reasonable number of columns to avoid overcrowding the plots
subset_cols_to_visualize = cols_to_visualize[:16] # Adjust the number as needed

n_cols = 4
n_rows = (len(subset_cols_to_visualize) + n_cols - 1) // n_cols

plt.figure(figsize=(20, n_rows * 5))

for i, col in enumerate(subset_cols_to_visualize):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Box Plot of {col}\n(Outliers shown as points)')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

""" Handle outliers


Apply a chosen technique (e.g., capping or removal) to handle the outliers in the dataset.

"""

df_capped = df.copy()

for col in numerical_cols:
    if col in outlier_bounds:
        lower_bound = outlier_bounds[col]['lower_bound']
        upper_bound = outlier_bounds[col]['upper_bound']
        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)

print("Outliers handled by capping.")

# Verify capping by checking min/max values for a few columns that previously had outliers
cols_to_verify = [col for col, count in outliers_count.items() if count > 0][:5] # Verify first 5 columns with outliers
print("\nVerifying min/max values after capping:")
for col in cols_to_verify:
    print(f"{col}: Min = {df_capped[col].min():.2f}, Max = {df_capped[col].max():.2f}")

df = df_capped

""" Verify outlier handling

Verify outlier handling by re-visualizing the data or checking descriptive statistics to confirm that outliers have been handled.

"""

# Select a subset of numerical columns to re-visualize
# Reuse the list cols_to_visualize if available, otherwise select some manually
if 'cols_to_visualize' in locals():
    subset_cols_to_visualize_capped = cols_to_visualize[:16] # Use the same subset as before for comparison
else:
    # Manually select some columns that had a high number of outliers
    subset_cols_to_visualize_capped = ['01_CURR_ACC', '02_TIME_DEPOSIT', '03_SAVINGS', '04_OTHER_DEPOSITS', '05_BANKS_DEPOSITS', '07_INTERBANKS_LOAN_PAYABLE', '08_CHEQUES_ISSUED', '09_PAY_ORDERS', '10_FOREIGN_DEPOSITS_AND_BORROWINGS', '11_OFF_BALSHEET_COMMITMENTS', '12_OTHER_LIABILITIES', '13_CASH', '14_CURRENT_ACC', '15_SMR_ACC', '16_FOREIGN CURRENCY', '18_BANKS_TZ']

# Create box plots for the selected columns in the capped DataFrame
n_cols = 4
n_rows = (len(subset_cols_to_visualize_capped) + n_cols - 1) // n_cols

plt.figure(figsize=(20, n_rows * 5))

for i, col in enumerate(subset_cols_to_visualize_capped):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Box Plot of {col} (Capped Data)')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

# Display descriptive statistics for a few of the selected columns to observe capping effect
cols_for_describe = subset_cols_to_visualize_capped[:5] # Choose first 5 for brevity
print("\nDescriptive statistics for a few capped columns:")
display(df[cols_for_describe].describe())

"""# Task
Analyze the correlation between variables through a heatmap and calculate VIF to check for multicollinearity.

## Calculate the correlation matrix

### Subtask:
Calculate the pairwise correlation between all numerical variables in the DataFrame.

**Reasoning**:
Calculate the pairwise correlation between all numerical variables in the DataFrame and display the resulting correlation matrix.
"""

correlation_matrix = df[numerical_cols].corr()
display(correlation_matrix)

"""## Visualize the correlation matrix

### Subtask:
Create a heatmap to visualize the correlation matrix.

**Reasoning**:
Create a heatmap to visualize the correlation matrix.
"""

plt.figure(figsize=(25, 20))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features', fontsize=20)
plt.show()

"""## Analyze the heatmap

### Subtask:
Interpret the heatmap to identify highly correlated variables.

**Reasoning**:
Based on the previously generated heatmap, I will examine it to identify pairs of variables with high positive or negative correlations (absolute correlation coefficient greater than 0.7) and summarize these observations.
"""

# Identify highly correlated pairs (absolute correlation > 0.7)
highly_correlated_pairs = []
# Iterate through the upper triangle of the correlation matrix to avoid duplicates
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.7:
            col1 = correlation_matrix.columns[i]
            col2 = correlation_matrix.columns[j]
            correlation_value = correlation_matrix.iloc[i, j]
            highly_correlated_pairs.append((col1, col2, correlation_value))

# Sort by absolute correlation value in descending order
highly_correlated_pairs.sort(key=lambda x: abs(x[2]), reverse=True)

print("Highly Correlated Variable Pairs (Absolute Correlation > 0.7):")
for pair in highly_correlated_pairs:
    print(f"{pair[0]} and {pair[1]}: {pair[2]:.4f}")

# Summarize observations
print("\nSummary of Observations:")
print("The heatmap and the list above show several pairs of variables with strong positive or negative correlations.")
print("Notable highly correlated pairs include:")
for pair in highly_correlated_pairs[:10]: # Print top 10 for brevity
    print(f"- {pair[0]} and {pair[1]} (Correlation: {pair[2]:.4f})")

"""**Reasoning**:
I have examined the heatmap and identified the highly correlated variable pairs. The next step is to summarize these findings as requested by the subtask instructions.


"""

print("\nInterpretation of the Correlation Heatmap:")
print("The heatmap visually confirms the presence of strong correlations among several numerical features. As observed from the list of highly correlated pairs (absolute correlation > 0.7), many of these relationships are positive, indicating that as the value of one variable increases, the value of the other tends to increase as well.")
print("Key observations include:")
print("- Strong positive correlations exist between total assets (F077_ASSETS_TOTAL), total liabilities (F125_LIAB_TOTAL), total liquid liabilities (XX_TOTAL_LIQUID_LIAB), and customer deposits (XX_CUSTOMER_DEPOSITS). This is expected as these are fundamental components of a bank's balance sheet and are likely to scale together with the size of the institution.")
print("- Liquid asset components such as current accounts (01_CURR_ACC), BOT balance (XX_BOT_BALANCE), balances in other banks (XX_BAL_IN_OTHER_BANKS), treasury bills (22_TREASURY_BILLS), and foreign currency holdings (10_FOREIGN_DEPOSITS_AND_BORROWINGS, 16_FOREIGN CURRENCY, 24_FOREIGN_CURRENCY) show high positive correlations with the total liquid assets (XX_TOTAL_LIQUID_ASSET) and other balance sheet items, which is also expected.")
print("- There is a very strong positive correlation between EWAQ_NPL and EWAQ_NPLsNetOfProvisions, suggesting that the provision for non-performing loans is closely related to the gross amount of NPLs.")
print("- The variables XX_MLA and MLA, which appear to be related to a liquidity measure and its classification, are highly correlated, as are MLA_CLASS2 and XX_MLA_CLASS2, which are likely different representations of the target variable or a related classification.")
print("- Some pairs, like MLA and MLA_CLASS2, and XX_MLA and XX_MLA_CLASS2, show strong negative correlations, which might indicate an inverse relationship in their scaling or definition.")
print("Overall, the heatmap highlights significant linear relationships among many of the financial metrics, particularly those representing different components of the bank's assets, liabilities, and liquidity. This multicollinearity among predictor variables should be considered in subsequent modeling steps.")

"""## Calculate vif

### Subtask:
Calculate the Variance Inflation Factor (VIF) for each numerical feature to quantify multicollinearity.

**Reasoning**:
Calculate VIF for each numerical feature, excluding the target variable, handle potential infinite values, store in a DataFrame, sort, and display the results.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd
import numpy as np

# Select numerical columns excluding the target variable
numerical_features = df.select_dtypes(include=np.number).columns.tolist()
target_variable = 'XX_MLA_CLASS2'

if target_variable in numerical_features:
    numerical_features.remove(target_variable)

# Create a DataFrame with only the numerical features for VIF calculation
X = df[numerical_features]


X = X.replace([np.inf, -np.inf], np.finfo(np.float64).max) # Replace with max float value


# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF_Value"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Sort the DataFrame by VIF_Value in descending order
vif_data = vif_data.sort_values(by="VIF_Value", ascending=False)

# Display the VIF values
display(vif_data)

"""Analyze vif values


Interpret the VIF values to identify features with high multicollinearity.

**Reasoning**:
Interpret the VIF values to identify features with high multicollinearity, relate findings to the correlation heatmap, and summarize implications for model building.
"""

print("--- Interpretation of VIF Values ---")

# Define a common threshold for high VIF (e.g., 5 or 10)
vif_threshold = 10

# Identify features with VIF above the threshold
high_vif_features = vif_data[vif_data['VIF_Value'] > vif_threshold]

print(f"\nFeatures with VIF values greater than {vif_threshold}:")
display(high_vif_features)

print("\nDiscussion of Multicollinearity:")
print(f"According to the VIF values, several features exhibit significant multicollinearity (VIF > {vif_threshold}).")
print("The features with the highest VIF values are:")
for index, row in high_vif_features.iterrows():
    print(f"- {row['Feature']} (VIF: {row['VIF_Value']:.2f})")

"""## Address multicollinearity (optional)

### Subtask:
Address multicollinearity by removing features with high VIF values.

**Reasoning**:
Identify features with high VIF and drop them from the DataFrame to address multicollinearity.
"""

# Identify features with VIF values above the threshold (e.g., 10)
vif_threshold = 10
high_vif_features_to_drop = vif_data[vif_data['VIF_Value'] > vif_threshold]['Feature'].tolist()



features_to_drop_vif = [
    'F077_ASSETS_TOTAL',
    'F125_LIAB_TOTAL',
    'XX_TOTAL_LIQUID_LIAB',
    'XX_CUSTOMER_DEPOSITS',
    'EWAQ_GrossLoans',
    'EWAQ_Capital',
    'XX_TOTAL_LIQUID_ASSET',
    '01_CURR_ACC',
    'XX_BOT_BALANCE',
    '19_BANKS_ABROAD',
    'XX_BAL_IN_OTHER_BANKS',
    # Based on the previous analysis, MLA and XX_MLA are highly correlated with VIFs > 9
    # Let's keep one of them, like XX_MLA, and drop MLA
    'MLA',
     # Features with NaN VIF value provide no variance and can be dropped
    '06_BORROWING_FROM_PUBLIC',
    '09_PAY_ORDERS',
    '17_OTHER DEPOSITS',
    '25_COMMERCIAL_BILLS',
    '26_PROMISSORY_NOTES'
]


# Drop the selected features from the DataFrame
print("Shape of DataFrame before dropping columns:", df.shape)
df_reduced_vif = df.drop(columns=features_to_drop_vif, errors='ignore')
print("Shape of DataFrame after dropping columns:", df_reduced_vif.shape)

# Display the first few rows of the updated DataFrame
display(df_reduced_vif.head())

# Update the main DataFrame variable to reflect the changes for subsequent tasks
df = df_reduced_vif

"""Feature Enginnering"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    features_df, y, test_size=0.33, random_state=42, stratify=y # Stratify 'y' for imbalanced data
)

print("Data split into training and testing sets.")
print(f"Training features shape: {X_train.shape}")
print(f"Testing features shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Testing target shape: {y_test.shape}")

"""Feature Scaling (using StandardScaler / Z-score)

Scale the features using StandardScaler. This is important for many machine learning algorithms as it standardizes the features by removing the mean and scaling to unit variance. This is equivalent to calculating the Z-score for each data point.
"""

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Features scaled using StandardScaler.")
print(f"Scaled training features shape: {X_train_scaled.shape}")
print(f"Scaled testing features shape: {X_test_scaled.shape}")

"""CLASS IMBALNCCE

"""

# Check the count of each unique value in the target variable
target_counts = df['XX_MLA_CLASS2'].value_counts()

print("Count of each class in the target variable (XX_MLA_CLASS2):")
display(target_counts)

from sklearn.utils import resample
import pandas as pd

# Select features and target from the original dataframe for balancing
# Assuming df_numeric is available and contains only numerical columns
df_numeric = df.select_dtypes(include=np.number) # Ensure df_numeric is defined

X_original = df_numeric.drop(columns=['XX_MLA_CLASS2', 'EWL_LIQUIDITY RATING', 'MLA_CLASS2'])
y_original = df_numeric['XX_MLA_CLASS2']

# Split the original data for creating the balanced training set
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
    X_original, y_original, test_size=0.33, random_state=42, stratify=y_original # Stratify for imbalanced data
)

# Scale the original features before balancing
scaler_orig = StandardScaler()
X_train_orig_scaled = scaler_orig.fit_transform(X_train_orig)
X_test_orig_scaled = scaler_orig.transform(X_test_orig)


# Combine scaled training features and target for easier sampling
train_df_scaled = pd.concat([pd.DataFrame(X_train_orig_scaled, columns=X_train_orig.columns), y_train_orig.reset_index(drop=True)], axis=1)

# Separate majority and minority classes
majority_class = train_df_scaled['XX_MLA_CLASS2'].value_counts().idxmax()
df_majority = train_df_scaled[train_df_scaled['XX_MLA_CLASS2'] == majority_class]
df_minority_list = [train_df_scaled[train_df_scaled['XX_MLA_CLASS2'] == cls] for cls in train_df_scaled['XX_MLA_CLASS2'].unique() if cls != majority_class]

# Determine the size of the majority class for upsampling
majority_class_size = len(df_majority)

# Upsample minority classes - only resample if the minority class dataframe is not empty
df_minority_upsampled = [resample(df_minority,
                                replace=True,     # sample with replacement
                                n_samples=majority_class_size,    # to match majority class size
                                random_state=42) # reproducible results
                        for df_minority in df_minority_list if not df_minority.empty] # Added check for empty dataframe


# Combine majority class with upsampled minority classes
# Ensure df_minority_upsampled is not empty before concatenating
if df_minority_upsampled:
    df_upsampled = pd.concat([df_majority] + df_minority_upsampled)
else:
    df_upsampled = df_majority # If no minority classes to upsample, just use the majority class data

# Shuffle the upsampled data
df_balanced_orig = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)

# Separate features and target from the balanced dataframe
X_train_balanced_orig = df_balanced_orig.drop('XX_MLA_CLASS2', axis=1)
y_train_balanced_orig = df_balanced_orig['XX_MLA_CLASS2']


print("Class imbalance handled using up-sampling on original features.")
print("Shape of original training data (scaled):", X_train_orig_scaled.shape)
print("Shape of balanced training data (original features):", X_train_balanced_orig.shape)
print("\nClass distribution in original training data:")
display(y_train_orig.value_counts().sort_index()) # Display sorted index for better comparison
print("\nClass distribution in balanced training data (original features):")
display(y_train_balanced_orig.value_counts().sort_index()) # Display sorted index for better comparison

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming y_train and y_train_balanced are available from previous cells

print("Class Distribution Before and After Up-sampling:")

# Plotting original imbalanced training data distribution
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot
sns.countplot(x=y_train, order=y_train.value_counts().index, palette='viridis')
plt.title('Original Imbalanced Training Data Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')

# Plotting balanced training data distribution
plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot
sns.countplot(x=y_train_balanced_orig, order=y_train_balanced_orig.value_counts().index, palette='viridis')
plt.title('Balanced Training Data Class Distribution (After Up-sampling)')
plt.xlabel('Class')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""APPLY ML ALOGORITHM

1. logistic regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig, y_original are available

# 1. Initialize the Logistic Regression model
lr_model_balanced_orig = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)

# 2. Train the Logistic Regression model on balanced training data
print("\nTraining Logistic Regression model...")
lr_model_balanced_orig.fit(X_train_balanced_orig, y_train_balanced_orig)
print("Logistic Regression model training complete.")

# 3. Make predictions and get probability estimates on the balanced training data
lr_preds_balanced_orig_train = lr_model_balanced_orig.predict(X_train_balanced_orig)
lr_probabilities_balanced_orig_train = lr_model_balanced_orig.predict_proba(X_train_balanced_orig)

# 4. Make predictions and get probability estimates on the scaled original testing data
lr_preds_balanced_orig_test = lr_model_balanced_orig.predict(X_test_orig_scaled)
lr_probabilities_balanced_orig_test = lr_model_balanced_orig.predict_proba(X_test_orig_scaled)

# Define classes for metrics calculation (using all unique classes from the original data)
ALL_CLASSES_ORIG = sorted(y_original.unique())

# 5. Calculate and print performance metrics on Balanced Training Data
print("--- Logistic Regression Performance Measures (Balanced Original Features - Training Data) ---")

accuracy_train_balanced_orig_lr = accuracy_score(y_train_balanced_orig, lr_preds_balanced_orig_train)
balanced_accuracy_train_balanced_orig_lr = balanced_accuracy_score(y_train_balanced_orig, lr_preds_balanced_orig_train)
precision_train_balanced_orig_lr = precision_score(y_train_balanced_orig, lr_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_lr = recall_score(y_train_balanced_orig, lr_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_lr = f1_score(y_train_balanced_orig, lr_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_lr = cohen_kappa_score(y_train_balanced_orig, lr_preds_balanced_orig_train)
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
auc_train_balanced_orig_lr = roc_auc_score(y_train_balanced_orig_binarized, lr_probabilities_balanced_orig_train, multi_class='ovr', average='macro')


print(f"Accuracy: {accuracy_train_balanced_orig_lr * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_lr * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_lr * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_lr * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_lr * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_lr * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_lr:.4f}")


# 6. Calculate and print performance metrics on Testing Data
print("\n--- Logistic Regression Performance Measures (Balanced Original Features - Testing Data) ---")

accuracy_test_balanced_orig_lr = accuracy_score(y_test_orig, lr_preds_balanced_orig_test)
balanced_accuracy_test_balanced_orig_lr = balanced_accuracy_score(y_test_orig, lr_preds_balanced_orig_test)
precision_test_balanced_orig_lr = precision_score(y_test_orig, lr_preds_balanced_orig_test, average='macro', zero_division=0)
recall_test_balanced_orig_lr = recall_score(y_test_orig, lr_preds_balanced_orig_test, average='macro', zero_division=0)
f1_test_balanced_orig_lr = f1_score(y_test_orig, lr_preds_balanced_orig_test, average='macro', zero_division=0)
kappa_test_balanced_orig_lr = cohen_kappa_score(y_test_orig, lr_preds_balanced_orig_test)


print(f"Accuracy: {accuracy_test_balanced_orig_lr * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_balanced_orig_lr * 100:.2f}%")
print(f"Precision (Macro): {precision_test_balanced_orig_lr * 100:.2f}%")
print(f"Recall (Macro): {recall_test_balanced_orig_lr * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_balanced_orig_lr * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_balanced_orig_lr:.4f}")

# Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
auc_test_balanced_orig_lr = roc_auc_score(y_test_orig_binarized, lr_probabilities_balanced_orig_test, multi_class='ovr', average='macro')
print(f"Area Under the Curve (AUC): {auc_test_balanced_orig_lr * 100:.2f}%")


# 7. Calculate and display the Confusion Matrix as a heatmap (Testing Data)
cm_balanced_orig_lr = confusion_matrix(y_test_orig, lr_preds_balanced_orig_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Testing Data):")
display(cm_balanced_orig_lr)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_lr, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Logistic Regression (Balanced Original Features - Testing Data)')
plt.show()

# 8. Calculate and print metrics derived from the Confusion Matrix (Testing Data)ACC
n_classes_balanced_orig_lr = cm_balanced_orig_lr.shape[0]
metrics_agg_balanced_orig_lr = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_lr):
    tp = cm_balanced_orig_lr[i, i]
    fn = np.sum(cm_balanced_orig_lr[i, :]) - tp
    fp = np.sum(cm_balanced_orig_lr[:, i]) - tp
    tn = np.sum(cm_balanced_orig_lr) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_lr['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_lr['specificity'] += specificity_class

sensitivity_macro_balanced_orig_lr = metrics_agg_balanced_orig_lr['sensitivity'] / n_classes_balanced_orig_lr
specificity_macro_balanced_orig_lr = metrics_agg_balanced_orig_lr['specificity'] / n_classes_balanced_orig_lr

type1_error_balanced_orig_lr = 1 - specificity_macro_balanced_orig_lr
type2_error_balanced_orig_lr = 1 - sensitivity_macro_balanced_orig_lr
g_mean_balanced_orig_lr = np.sqrt(sensitivity_macro_balanced_orig_lr * specificity_macro_balanced_orig_lr)
youdens_index_balanced_orig_lr = sensitivity_macro_balanced_orig_lr + specificity_macro_balanced_orig_lr - 1
# Avoid division by zero for nlir calculation
nlir_balanced_orig_lr = type2_error_balanced_orig_lr / specificity_macro_balanced_orig_lr if specificity_macro_balanced_orig_lr > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_lr = np.log((sensitivity_macro_balanced_orig_lr / (type1_error_balanced_orig_lr if type1_error_balanced_orig_lr > 0 else 1e-9)) * (specificity_macro_balanced_orig_lr / (type2_error_balanced_orig_lr if type2_error_balanced_orig_lr > 0 else 1e-9))) if (type1_error_balanced_orig_lr > 0 and type2_error_balanced_orig_lr > 0 and sensitivity_macro_balanced_orig_lr > 0 and specificity_macro_balanced_orig_lr > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_lr:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_lr:.4f}")
print(f"G-mean: {g_mean_balanced_orig_lr:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_lr:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_lr:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_lr:.4f}")


# 9. Compute and plot the ROC curves for each class (Testing Data)
fpr_lr = dict()
tpr_lr = dict()
roc_auc_lr = dict()
for i in range(len(ALL_CLASSES_ORIG)):
    # roc_curve expects binary labels, so we use the binarized y_test
    fpr_lr[i], tpr_lr[i], _ = roc_curve(y_test_orig_binarized[:, i], lr_probabilities_balanced_orig_test[:, i])
    roc_auc_lr[i] = auc(fpr_lr[i], tpr_lr[i])

# Plot the ROC curves
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
    plt.plot(fpr_lr[i], tpr_lr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_lr[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Logistic Regression (Balanced Original Features)')
plt.legend(loc="lower right")
plt.show()


# Calculate and display the Confusion Matrix as a heatmap (Training Data)
cm_balanced_orig_train_lr = confusion_matrix(y_train_balanced_orig, lr_preds_balanced_orig_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Training Data):")
display(cm_balanced_orig_train_lr)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_train_lr, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Logistic Regression (Balanced Original Features - Training Data)')
plt.show()

# Calculate and print metrics derived from the Confusion Matrix (Training Data)
n_classes_balanced_orig_train_lr = cm_balanced_orig_train_lr.shape[0]
metrics_agg_balanced_orig_train_lr = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_train_lr):
    tp = cm_balanced_orig_train_lr[i, i]
    fn = np.sum(cm_balanced_orig_train_lr[i, :]) - tp
    fp = np.sum(cm_balanced_orig_train_lr[:, i]) - tp
    tn = np.sum(cm_balanced_orig_train_lr) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_train_lr['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_train_lr['specificity'] += specificity_class

sensitivity_macro_balanced_orig_train_lr = metrics_agg_balanced_orig_train_lr['sensitivity'] / n_classes_balanced_orig_train_lr
specificity_macro_balanced_orig_train_lr = metrics_agg_balanced_orig_train_lr['specificity'] / n_classes_balanced_orig_train_lr

type1_error_balanced_orig_train_lr = 1 - specificity_macro_balanced_orig_train_lr
type2_error_balanced_orig_train_lr = 1 - sensitivity_macro_balanced_orig_train_lr
g_mean_balanced_orig_train_lr = np.sqrt(sensitivity_macro_balanced_orig_train_lr * specificity_macro_balanced_orig_train_lr)
youdens_index_balanced_orig_train_lr = sensitivity_macro_balanced_orig_train_lr + specificity_macro_balanced_orig_train_lr - 1

# Avoid division by zero for nlir calculation
nlir_balanced_orig_train_lr = type2_error_balanced_orig_train_lr / specificity_macro_balanced_orig_train_lr if specificity_macro_balanced_orig_train_lr > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_train_lr = np.log((sensitivity_macro_balanced_orig_train_lr / (type1_error_balanced_orig_train_lr if type1_error_balanced_orig_train_lr > 0 else 1e-9)) * (specificity_macro_balanced_orig_train_lr / (type2_error_balanced_orig_train_lr if type2_error_balanced_orig_train_lr > 0 else 1e-9))) if (type1_error_balanced_orig_train_lr > 0 and type2_error_balanced_orig_train_lr > 0 and sensitivity_macro_balanced_orig_train_lr > 0 and specificity_macro_balanced_orig_train_lr > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_train_lr:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_train_lr:.4f}")
print(f"G-mean: {g_mean_balanced_orig_train_lr:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_train_lr:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_train_lr:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_train_lr:.4f}")

# Compute ROC curve and ROC area for each class (Training Data)
fpr_lr_train = dict()
tpr_lr_train = dict()
roc_auc_lr_train = dict()
for i in range(len(ALL_CLASSES_ORIG)):
    fpr_lr_train[i], tpr_lr_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], lr_probabilities_balanced_orig_train[:, i])
    roc_auc_lr_train[i] = auc(fpr_lr_train[i], tpr_lr_train[i])

# Plot the ROC curves (Training Data)
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
    plt.plot(fpr_lr_train[i], tpr_lr_train[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_lr_train[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Logistic Regression (Balanced Original Features - Training Data)')
plt.legend(loc="lower right")
plt.show()

"""2.DECCSION TREE"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig, y_original are available

# 1. Initialize a Decision Tree Classifier
dt_model_balanced_orig = DecisionTreeClassifier(random_state=42)

# 2. Train the Decision Tree model on the balanced original training data
print("\nTraining Decision Tree model...")
dt_model_balanced_orig.fit(X_train_balanced_orig, y_train_balanced_orig)
print("Decision Tree model training complete.")


# 3. Make predictions and get probability estimates on the balanced training data
dt_preds_balanced_orig_train = dt_model_balanced_orig.predict(X_train_balanced_orig)
dt_probabilities_balanced_orig_train = dt_model_balanced_orig.predict_proba(X_train_balanced_orig)


# 4. Make predictions and get probability estimates on the scaled original testing data
dt_preds_balanced_orig_test = dt_model_balanced_orig.predict(X_test_orig_scaled)
dt_probabilities_balanced_orig_test = dt_model_balanced_orig.predict_proba(X_test_orig_scaled)

# Define classes for metrics calculation (using all unique classes from the original data)
ALL_CLASSES_ORIG = sorted(y_original.unique())

# 5. Calculate and print performance metrics on Balanced Training Data
print("--- Decision Tree Performance Measures (Balanced Original Features - Training Data) ---")

accuracy_train_balanced_orig_dt = accuracy_score(y_train_balanced_orig, dt_preds_balanced_orig_train)
balanced_accuracy_train_balanced_orig_dt = balanced_accuracy_score(y_train_balanced_orig, dt_preds_balanced_orig_train)
precision_train_balanced_orig_dt = precision_score(y_train_balanced_orig, dt_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_dt = recall_score(y_train_balanced_orig, dt_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_dt = f1_score(y_train_balanced_orig, dt_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_dt = cohen_kappa_score(y_train_balanced_orig, dt_preds_balanced_orig_train)
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
auc_train_balanced_orig_dt = roc_auc_score(y_train_balanced_orig_binarized, dt_probabilities_balanced_orig_train, multi_class='ovr', average='macro')


print(f"Accuracy: {accuracy_train_balanced_orig_dt * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_dt * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_dt * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_dt * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_dt * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_dt * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_dt:.4f}")


# 6. Calculate and print performance metrics on Testing Data
print("\n--- Decision Tree Performance Measures (Balanced Original Features - Testing Data) ---")

accuracy_test_balanced_orig_dt = accuracy_score(y_test_orig, dt_preds_balanced_orig_test)
balanced_accuracy_test_balanced_orig_dt = balanced_accuracy_score(y_test_orig, dt_preds_balanced_orig_test)
precision_test_balanced_orig_dt = precision_score(y_test_orig, dt_preds_balanced_orig_test, average='macro', zero_division=0)
recall_test_balanced_orig_dt = recall_score(y_test_orig, dt_preds_balanced_orig_test, average='macro', zero_division=0)
f1_test_balanced_orig_dt = f1_score(y_test_orig, dt_preds_balanced_orig_test, average='macro', zero_division=0)
kappa_test_balanced_orig_dt = cohen_kappa_score(y_test_orig, dt_preds_balanced_orig_test)


print(f"Accuracy: {accuracy_test_balanced_orig_dt * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_balanced_orig_dt * 100:.2f}%")
print(f"Precision (Macro): {precision_test_balanced_orig_dt * 100:.2f}%")
print(f"Recall (Macro): {recall_test_balanced_orig_dt * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_balanced_orig_dt * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_balanced_orig_dt:.4f}")

# Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
# Check if there are at least two classes present in y_test_orig_binarized to calculate AUC
if y_test_orig_binarized.shape[1] >= 2:
    auc_test_balanced_orig_dt = roc_auc_score(y_test_orig_binarized, dt_probabilities_balanced_orig_test, multi_class='ovr', average='macro')
    print(f"Area Under the Curve (AUC): {auc_test_balanced_orig_dt * 100:.2f}%")
else:
    print("AUC cannot be calculated as there is only one class in the test set.")


# 7. Calculate and display the Confusion Matrix as a heatmap (Testing Data)
cm_balanced_orig_dt = confusion_matrix(y_test_orig, dt_preds_balanced_orig_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Testing Data):")
display(cm_balanced_orig_dt)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_dt, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Decision Tree (Balanced Original Features - Testing Data)')
plt.show()


# 8. Calculate and print metrics derived from the Confusion Matrix (Testing Data)
n_classes_balanced_orig_dt = cm_balanced_orig_dt.shape[0]
metrics_agg_balanced_orig_dt = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_dt):
    tp = cm_balanced_orig_dt[i, i]
    fn = np.sum(cm_balanced_orig_dt[i, :]) - tp
    fp = np.sum(cm_balanced_orig_dt[:, i]) - tp
    tn = np.sum(cm_balanced_orig_dt) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_dt['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_dt['specificity'] += specificity_class

sensitivity_macro_balanced_orig_dt = metrics_agg_balanced_orig_dt['sensitivity'] / n_classes_balanced_orig_dt
specificity_macro_balanced_orig_dt = metrics_agg_balanced_orig_dt['specificity'] / n_classes_balanced_orig_dt

type1_error_balanced_orig_dt = 1 - specificity_macro_balanced_orig_dt
type2_error_balanced_orig_dt = 1 - sensitivity_macro_balanced_orig_dt
g_mean_balanced_orig_dt = np.sqrt(sensitivity_macro_balanced_orig_dt * specificity_macro_balanced_orig_dt)
youdens_index_balanced_orig_dt = sensitivity_macro_balanced_orig_dt + specificity_macro_balanced_orig_dt - 1
# Avoid division by zero for nlir calculation
nlir_balanced_orig_dt = type2_error_balanced_orig_dt / specificity_macro_balanced_orig_dt if specificity_macro_balanced_orig_dt > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_dt = np.log((sensitivity_macro_balanced_orig_dt / (type1_error_balanced_orig_dt if type1_error_balanced_orig_dt > 0 else 1e-9)) * (specificity_macro_balanced_orig_dt / (type2_error_balanced_orig_dt if type2_error_balanced_orig_dt > 0 else 1e-9))) if (type1_error_balanced_orig_dt > 0 and type2_error_balanced_orig_dt > 0 and sensitivity_macro_balanced_orig_dt > 0 and specificity_macro_balanced_orig_dt > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_dt:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_dt:.4f}")
print(f"G-mean: {g_mean_balanced_orig_dt:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_dt:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_dt:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_dt:.4f}")


# 9. Compute and plot the ROC curves for each class (Testing Data)
# Check if there are at least two classes present in y_test_orig_binarized to plot ROC curves
if y_test_orig_binarized.shape[1] >= 2:
    fpr_dt = dict()
    tpr_dt = dict()
    roc_auc_dt = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        # roc_curve expects binary labels, so we use the binarized y_test
        fpr_dt[i], tpr_dt[i], _ = roc_curve(y_test_orig_binarized[:, i], dt_probabilities_balanced_orig_test[:, i])
        roc_auc_dt[i] = auc(fpr_dt[i], tpr_dt[i])

    # Plot the ROC curves
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_dt[i], tpr_dt[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_dt[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - Decision Tree (Balanced Original Features)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves cannot be plotted as there is only one class in the test set.")


# Calculate and display the Confusion Matrix as a heatmap (Training Data)
cm_balanced_orig_train_dt = confusion_matrix(y_train_balanced_orig, dt_preds_balanced_orig_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Training Data):")
display(cm_balanced_orig_train_dt)

# 10. Calculate and print metrics derived from the Confusion Matrix (Training Data)
n_classes_balanced_orig_train_dt = cm_balanced_orig_train_dt.shape[0]
metrics_agg_balanced_orig_train_dt = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_train_dt):
    tp = cm_balanced_orig_train_dt[i, i]
    fn = np.sum(cm_balanced_orig_train_dt[i, :]) - tp
    fp = np.sum(cm_balanced_orig_train_dt[:, i]) - tp
    tn = np.sum(cm_balanced_orig_train_dt) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_train_dt['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_train_dt['specificity'] += specificity_class

sensitivity_macro_balanced_orig_train_dt = metrics_agg_balanced_orig_train_dt['sensitivity'] / n_classes_balanced_orig_train_dt
specificity_macro_balanced_orig_train_dt = metrics_agg_balanced_orig_train_dt['specificity'] / n_classes_balanced_orig_train_dt

type1_error_balanced_orig_train_dt = 1 - specificity_macro_balanced_orig_train_dt
type2_error_balanced_orig_train_dt = 1 - sensitivity_macro_balanced_orig_train_dt
g_mean_balanced_orig_train_dt = np.sqrt(sensitivity_macro_balanced_orig_train_dt * specificity_macro_balanced_orig_train_dt)
youdens_index_balanced_orig_train_dt = sensitivity_macro_balanced_orig_train_dt + specificity_macro_balanced_orig_train_dt - 1

# Avoid division by zero for nlir calculation
nlir_balanced_orig_train_dt = type2_error_balanced_orig_train_dt / specificity_macro_balanced_orig_train_dt if specificity_macro_balanced_orig_train_dt > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_train_dt = np.log((sensitivity_macro_balanced_orig_train_dt / (type1_error_balanced_orig_train_dt if type1_error_balanced_orig_train_dt > 0 else 1e-9)) * (specificity_macro_balanced_orig_train_dt / (type2_error_balanced_orig_train_dt if type2_error_balanced_orig_train_dt > 0 else 1e-9))) if (type1_error_balanced_orig_train_dt > 0 and type2_error_balanced_orig_train_dt > 0 and sensitivity_macro_balanced_orig_train_dt > 0 and specificity_macro_balanced_orig_train_dt > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_train_dt:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_train_dt:.4f}")
print(f"G-mean: {g_mean_balanced_orig_train_dt:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_train_dt:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_train_dt:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_train_dt:.4f}")


# Compute ROC curve and ROC area for each class (Training Data)
# Check if there are at least two classes present in y_train_balanced_orig_binarized to plot ROC curves
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
if y_train_balanced_orig_binarized.shape[1] >= 2:
    fpr_dt_train = dict()
    tpr_dt_train = dict()
    roc_auc_dt_train = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        fpr_dt_train[i], tpr_dt_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], dt_probabilities_balanced_orig_train[:, i])
        roc_auc_dt_train[i] = auc(fpr_dt_train[i], tpr_dt_train[i])

    # Plot the ROC curves (Training Data)
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_dt_train[i], tpr_dt_train[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_dt_train[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - Decision Tree (Balanced Original Features - Training Data)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves for training data cannot be plotted as there is only one class in the balanced training set.")

"""3.xg boosting"""

import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig, y_original are available

# 1. Initialize an XGBoost Classifier
# For multi-class classification, 'objective' should be 'multi:softmax' or 'multi:softprob'
# 'multi:softmax' outputs the predicted class index, 'multi:softprob' outputs probabilities
# We need probabilities for AUC calculation, so we'll use 'multi:softprob'
# The number of classes needs to be specified for multi-class objectives
# XGBoost expects class labels to be 0-indexed, so we need to adjust y_train_balanced_orig and y_test_orig
y_train_balanced_orig_xgb = y_train_balanced_orig - 1
y_test_orig_xgb = y_test_orig - 1

# Determine the number of classes from the original data for 'num_class'
num_classes_orig = len(ALL_CLASSES_ORIG)

xgb_model_balanced_orig = xgb.XGBClassifier(objective='multi:softprob',
                                            num_class=num_classes_orig,
                                            eval_metric='mlogloss', # Evaluation metric for multi-class
                                            use_label_encoder=False, # Deprecated, set to False
                                            random_state=42)

# 2. Train the XGBoost model on the balanced original training data
print("\nTraining XGBoost model...")
xgb_model_balanced_orig.fit(X_train_balanced_orig, y_train_balanced_orig_xgb)
print("XGBoost model training complete.")

# 3. Make predictions and get probability estimates on the balanced training data
xgb_preds_balanced_orig_train_raw = xgb_model_balanced_orig.predict(X_train_balanced_orig)
xgb_preds_balanced_orig_train = xgb_preds_balanced_orig_train_raw + 1 # Convert back to original class labels
xgb_probabilities_balanced_orig_train = xgb_model_balanced_orig.predict_proba(X_train_balanced_orig)

# 4. Make predictions and get probability estimates on the scaled original testing data
xgb_preds_balanced_orig_test_raw = xgb_model_balanced_orig.predict(X_test_orig_scaled)
xgb_preds_balanced_orig_test = xgb_preds_balanced_orig_test_raw + 1 # Convert back to original class labels
xgb_probabilities_balanced_orig_test = xgb_model_balanced_orig.predict_proba(X_test_orig_scaled)


# Define classes for metrics calculation (using all unique classes from the original data)
# ALL_CLASSES_ORIG is assumed to be defined from previous cells

# 5. Calculate and print performance metrics on Balanced Training Data
print("--- XGBoost Performance Measures (Balanced Original Features - Training Data) ---")

accuracy_train_balanced_orig_xgb = accuracy_score(y_train_balanced_orig, xgb_preds_balanced_orig_train)
balanced_accuracy_train_balanced_orig_xgb = balanced_accuracy_score(y_train_balanced_orig, xgb_preds_balanced_orig_train)
precision_train_balanced_orig_xgb = precision_score(y_train_balanced_orig, xgb_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_xgb = recall_score(y_train_balanced_orig, xgb_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_xgb = f1_score(y_train_balanced_orig, xgb_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_xgb = cohen_kappa_score(y_train_balanced_orig, xgb_preds_balanced_orig_train)
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
auc_train_balanced_orig_xgb = roc_auc_score(y_train_balanced_orig_binarized, xgb_probabilities_balanced_orig_train, multi_class='ovr', average='macro')


print(f"Accuracy: {accuracy_train_balanced_orig_xgb * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_xgb * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_xgb * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_xgb * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_xgb * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_xgb * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_xgb:.4f}")


# 6. Calculate and print performance metrics on Testing Data
print("\n--- XGBoost Performance Measures (Balanced Original Features - Testing Data) ---")

accuracy_test_balanced_orig_xgb = accuracy_score(y_test_orig, xgb_preds_balanced_orig_test)
balanced_accuracy_test_balanced_orig_xgb = balanced_accuracy_score(y_test_orig, xgb_preds_balanced_orig_test)
precision_test_balanced_orig_xgb = precision_score(y_test_orig, xgb_preds_balanced_orig_test, average='macro', zero_division=0)
recall_test_balanced_orig_xgb = recall_score(y_test_orig, xgb_preds_balanced_orig_test, average='macro', zero_division=0)
f1_test_balanced_orig_xgb = f1_score(y_test_orig, xgb_preds_balanced_orig_test, average='macro', zero_division=0)
kappa_test_balanced_orig_xgb = cohen_kappa_score(y_test_orig, xgb_preds_balanced_orig_test)


print(f"Accuracy: {accuracy_test_balanced_orig_xgb * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_balanced_orig_xgb * 100:.2f}%")
print(f"Precision (Macro): {precision_test_balanced_orig_xgb * 100:.2f}%")
print(f"Recall (Macro): {recall_test_balanced_orig_xgb * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_balanced_orig_xgb * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_balanced_orig_xgb:.4f}")

# Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
# Check if there are at least two classes present in y_test_orig_binarized to calculate AUC
if y_test_orig_binarized.shape[1] >= 2:
    auc_test_balanced_orig_xgb = roc_auc_score(y_test_orig_binarized, xgb_probabilities_balanced_orig_test, multi_class='ovr', average='macro')
    print(f"Area Under the Curve (AUC): {auc_test_balanced_orig_xgb * 100:.2f}%")
else:
    print("AUC cannot be calculated as there is only one class in the test set.")


# 7. Calculate and display the Confusion Matrix as a heatmap (Testing Data)
cm_balanced_orig_xgb = confusion_matrix(y_test_orig, xgb_preds_balanced_orig_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Testing Data):")
display(cm_balanced_orig_xgb)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - XGBoost (Balanced Original Features - Testing Data)')
plt.show()


# 8. Calculate and print metrics derived from the Confusion Matrix (Testing Data)
n_classes_balanced_orig_xgb = cm_balanced_orig_xgb.shape[0]
metrics_agg_balanced_orig_xgb = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_xgb):
    tp = cm_balanced_orig_xgb[i, i]
    fn = np.sum(cm_balanced_orig_xgb[i, :]) - tp
    fp = np.sum(cm_balanced_orig_xgb[:, i]) - tp
    tn = np.sum(cm_balanced_orig_xgb) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_xgb['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_xgb['specificity'] += specificity_class

sensitivity_macro_balanced_orig_xgb = metrics_agg_balanced_orig_xgb['sensitivity'] / n_classes_balanced_orig_xgb
specificity_macro_balanced_orig_xgb = metrics_agg_balanced_orig_xgb['specificity'] / n_classes_balanced_orig_xgb

type1_error_balanced_orig_xgb = 1 - specificity_macro_balanced_orig_xgb
type2_error_balanced_orig_xgb = 1 - sensitivity_macro_balanced_orig_xgb
g_mean_balanced_orig_xgb = np.sqrt(sensitivity_macro_balanced_orig_xgb * specificity_macro_balanced_orig_xgb)
youdens_index_balanced_orig_xgb = sensitivity_macro_balanced_orig_xgb + specificity_macro_balanced_orig_xgb - 1
# Avoid division by zero for nlir calculation
nlir_balanced_orig_xgb = type2_error_balanced_orig_xgb / specificity_macro_balanced_orig_xgb if specificity_macro_balanced_orig_xgb > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_xgb = np.log((sensitivity_macro_balanced_orig_xgb / (type1_error_balanced_orig_xgb if type1_error_balanced_orig_xgb > 0 else 1e-9)) * (specificity_macro_balanced_orig_xgb / (type2_error_balanced_orig_xgb if type2_error_balanced_orig_xgb > 0 else 1e-9))) if (type1_error_balanced_orig_xgb > 0 and type2_error_balanced_orig_xgb > 0 and sensitivity_macro_balanced_orig_xgb > 0 and specificity_macro_balanced_orig_xgb > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_xgb:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_xgb:.4f}")
print(f"G-mean: {g_mean_balanced_orig_xgb:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_xgb:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_xgb:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_xgb:.4f}")

# 9. Compute and plot the ROC curves for each class (Testing Data)
# Check if there are at least two classes present in y_test_orig_binarized to plot ROC curves
if y_test_orig_binarized.shape[1] >= 2:
    fpr_xgb = dict()
    tpr_xgb = dict()
    roc_auc_xgb = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        # roc_curve expects binary labels, so we use the binarized y_test
        fpr_xgb[i], tpr_xgb[i], _ = roc_curve(y_test_orig_binarized[:, i], xgb_probabilities_balanced_orig_test[:, i])
        roc_auc_xgb[i] = auc(fpr_xgb[i], tpr_xgb[i])

    # Plot the ROC curves
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_xgb[i], tpr_xgb[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_xgb[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - XGBoost (Balanced Original Features)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves cannot be plotted as there is only one class in the test set.")

# Calculate and display the Confusion Matrix as a heatmap (Training Data)
cm_balanced_orig_train_xgb = confusion_matrix(y_train_balanced_orig, xgb_preds_balanced_orig_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Training Data):")
display(cm_balanced_orig_train_xgb)


# 10. Calculate and print metrics derived from the Confusion Matrix (Training Data)
n_classes_balanced_orig_train_xgb = cm_balanced_orig_train_xgb.shape[0]
metrics_agg_balanced_orig_train_xgb = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_train_xgb):
    tp = cm_balanced_orig_train_xgb[i, i]
    fn = np.sum(cm_balanced_orig_train_xgb[i, :]) - tp
    fp = np.sum(cm_balanced_orig_train_xgb[:, i]) - tp
    tn = np.sum(cm_balanced_orig_train_xgb) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_train_xgb['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_train_xgb['specificity'] += specificity_class

sensitivity_macro_balanced_orig_train_xgb = metrics_agg_balanced_orig_train_xgb['sensitivity'] / n_classes_balanced_orig_train_xgb
specificity_macro_balanced_orig_train_xgb = metrics_agg_balanced_orig_train_xgb['specificity'] / n_classes_balanced_orig_train_xgb

type1_error_balanced_orig_train_xgb = 1 - specificity_macro_balanced_orig_train_xgb
type2_error_balanced_orig_train_xgb = 1 - sensitivity_macro_balanced_orig_train_xgb
g_mean_balanced_orig_train_xgb = np.sqrt(sensitivity_macro_balanced_orig_train_xgb * specificity_macro_balanced_orig_train_xgb)
youdens_index_balanced_orig_train_xgb = sensitivity_macro_balanced_orig_train_xgb + specificity_macro_balanced_orig_train_xgb - 1

# Avoid division by zero for nlir calculation
nlir_balanced_orig_train_xgb = type2_error_balanced_orig_train_xgb / specificity_macro_balanced_orig_train_xgb if specificity_macro_balanced_orig_train_xgb > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_train_xgb = np.log((sensitivity_macro_balanced_orig_train_xgb / (type1_error_balanced_orig_train_xgb if type1_error_balanced_orig_train_xgb > 0 else 1e-9)) * (specificity_macro_balanced_orig_train_xgb / (type2_error_balanced_orig_train_xgb if type2_error_balanced_orig_train_xgb > 0 else 1e-9))) if (type1_error_balanced_orig_train_xgb > 0 and type2_error_balanced_orig_train_xgb > 0 and sensitivity_macro_balanced_orig_train_xgb > 0 and specificity_macro_balanced_orig_train_xgb > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_train_xgb:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_train_xgb:.4f}")
print(f"G-mean: {g_mean_balanced_orig_train_xgb:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_train_xgb:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_train_xgb:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_train_xgb:.4f}")


# Compute ROC curve and ROC area for each class (Training Data)
# Check if there are at least two classes present in y_train_balanced_orig_binarized to plot ROC curves
if y_train_balanced_orig_binarized.shape[1] >= 2:
    fpr_xgb_train = dict()
    tpr_xgb_train = dict()
    roc_auc_xgb_train = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        fpr_xgb_train[i], tpr_xgb_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], xgb_probabilities_balanced_orig_train[:, i])
        roc_auc_xgb_train[i] = auc(fpr_xgb_train[i], tpr_xgb_train[i])

    # Plot the ROC curves (Training Data)
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_xgb_train[i], tpr_xgb_train[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_xgb_train[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - XGBoost (Balanced Original Features - Training Data)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves for training data cannot be plotted as there is only one class in the balanced training set.")

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig, y_original are available
# SVM with a radial basis function (RBF) kernel is a common choice
# For multi-class classification with SVC, the 'decision_function_shape' parameter is relevant ('ovr' or 'ovo')
# We need probability estimates for AUC, so we set probability=True
# SVC with probability=True can be slower due to internal cross-validation

# 1. Initialize the SVM model
svm_model_balanced_orig = SVC(kernel='rbf', probability=True, random_state=42)

# 2. Train the SVM model on the balanced original training data
print("\nTraining SVM model...")
# SVM expects class labels to be 0-indexed or 1-indexed. Using original labels (1-5) should be fine.
svm_model_balanced_orig.fit(X_train_balanced_orig, y_train_balanced_orig)
print("SVM model training complete.")

# 3. Make predictions and get probability estimates on the balanced training data
svm_preds_balanced_orig_train = svm_model_balanced_orig.predict(X_train_balanced_orig)
svm_probabilities_balanced_orig_train = svm_model_balanced_orig.predict_proba(X_train_balanced_orig)


# 4. Make predictions and get probability estimates on the scaled original testing data
svm_preds_balanced_orig_test = svm_model_balanced_orig.predict(X_test_orig_scaled)
svm_probabilities_balanced_orig_test = svm_model_balanced_orig.predict_proba(X_test_orig_scaled)

# Define classes for metrics calculation (using all unique classes from the original data)
# ALL_CLASSES_ORIG is assumed to be defined from previous cells

# 5. Calculate and print performance metrics on Balanced Training Data
print("--- SVM Performance Measures (Balanced Original Features - Training Data) ---")

accuracy_train_balanced_orig_svm = accuracy_score(y_train_balanced_orig, svm_preds_balanced_orig_train)
balanced_accuracy_train_balanced_orig_svm = balanced_accuracy_score(y_train_balanced_orig, svm_preds_balanced_orig_train)
precision_train_balanced_orig_svm = precision_score(y_train_balanced_orig, svm_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_svm = recall_score(y_train_balanced_orig, svm_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_svm = f1_score(y_train_balanced_orig, svm_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_svm = cohen_kappa_score(y_train_balanced_orig, svm_preds_balanced_orig_train)
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
auc_train_balanced_orig_svm = roc_auc_score(y_train_balanced_orig_binarized, svm_probabilities_balanced_orig_train, multi_class='ovr', average='macro')


print(f"Accuracy: {accuracy_train_balanced_orig_svm * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_svm * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_svm * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_svm * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_svm * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_svm * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_svm:.4f}")


# 6. Calculate and print performance metrics on Testing Data
print("\n--- SVM Performance Measures (Balanced Original Features - Testing Data) ---")

accuracy_test_balanced_orig_svm = accuracy_score(y_test_orig, svm_preds_balanced_orig_test)
balanced_accuracy_test_balanced_orig_svm = balanced_accuracy_score(y_test_orig, svm_preds_balanced_orig_test)
precision_test_balanced_orig_svm = precision_score(y_test_orig, svm_preds_balanced_orig_test, average='macro', zero_division=0)
recall_test_balanced_orig_svm = recall_score(y_test_orig, svm_preds_balanced_orig_test, average='macro', zero_division=0)
f1_test_balanced_orig_svm = f1_score(y_test_orig, svm_preds_balanced_orig_test, average='macro', zero_division=0)
kappa_test_balanced_orig_svm = cohen_kappa_score(y_test_orig, svm_preds_balanced_orig_test)


print(f"Accuracy: {accuracy_test_balanced_orig_svm * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_balanced_orig_svm * 100:.2f}%")
print(f"Precision (Macro): {precision_test_balanced_orig_svm * 100:.2f}%")
print(f"Recall (Macro): {recall_test_balanced_orig_svm * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_balanced_orig_svm * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_balanced_orig_svm:.4f}")

# Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
# Check if there are at least two classes present in y_test_orig_binarized to calculate AUC
if y_test_orig_binarized.shape[1] >= 2:
    auc_test_balanced_orig_svm = roc_auc_score(y_test_orig_binarized, svm_probabilities_balanced_orig_test, multi_class='ovr', average='macro')
    print(f"Area Under the Curve (AUC): {auc_test_balanced_orig_svm * 100:.2f}%")
else:
    print("AUC cannot be calculated as there is only one class in the test set.")


# 7. Calculate and display the Confusion Matrix as a heatmap (Testing Data)
cm_balanced_orig_svm = confusion_matrix(y_test_orig, svm_preds_balanced_orig_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Testing Data):")
display(cm_balanced_orig_svm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_svm, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - SVM (Balanced Original Features - Testing Data)')
plt.show()


# 8. Calculate and print metrics derived from the Confusion Matrix (Testing Data)
n_classes_balanced_orig_svm = cm_balanced_orig_svm.shape[0]
metrics_agg_balanced_orig_svm = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_svm):
    tp = cm_balanced_orig_svm[i, i]
    fn = np.sum(cm_balanced_orig_svm[i, :]) - tp
    fp = np.sum(cm_balanced_orig_svm[:, i]) - tp
    tn = np.sum(cm_balanced_orig_svm) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_svm['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_svm['specificity'] += specificity_class

sensitivity_macro_balanced_orig_svm = metrics_agg_balanced_orig_svm['sensitivity'] / n_classes_balanced_orig_svm
specificity_macro_balanced_orig_svm = metrics_agg_balanced_orig_svm['specificity'] / n_classes_balanced_orig_svm

type1_error_balanced_orig_svm = 1 - specificity_macro_balanced_orig_svm
type2_error_balanced_orig_svm = 1 - sensitivity_macro_balanced_orig_svm
g_mean_balanced_orig_svm = np.sqrt(sensitivity_macro_balanced_orig_svm * specificity_macro_balanced_orig_svm)
youdens_index_balanced_orig_svm = sensitivity_macro_balanced_orig_svm + specificity_macro_balanced_orig_svm - 1
# Avoid division by zero for nlir calculation
nlir_balanced_orig_svm = type2_error_balanced_orig_svm / specificity_macro_balanced_orig_svm if specificity_macro_balanced_orig_svm > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_svm = np.log((sensitivity_macro_balanced_orig_svm / (type1_error_balanced_orig_svm if type1_error_balanced_orig_svm > 0 else 1e-9)) * (specificity_macro_balanced_orig_svm / (type2_error_balanced_orig_svm if type2_error_balanced_orig_svm > 0 else 1e-9))) if (type1_error_balanced_orig_svm > 0 and type2_error_balanced_orig_svm > 0 and sensitivity_macro_balanced_orig_svm > 0 and specificity_macro_balanced_orig_svm > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_svm:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_svm:.4f}")
print(f"G-mean: {g_mean_balanced_orig_svm:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_svm:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_svm:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_svm:.4f}")


# 9. Compute and plot the ROC curves for each class (Testing Data)
# Check if there are at least two classes present in y_test_orig_binarized to plot ROC curves
if y_test_orig_binarized.shape[1] >= 2:
    fpr_svm = dict()
    tpr_svm = dict()
    roc_auc_svm = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        # roc_curve expects binary labels, so we use the binarized y_test
        fpr_svm[i], tpr_svm[i], _ = roc_curve(y_test_orig_binarized[:, i], svm_probabilities_balanced_orig_test[:, i])
        roc_auc_svm[i] = auc(fpr_svm[i], tpr_svm[i])

    # Plot the ROC curves
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_svm[i], tpr_svm[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_svm[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - SVM (Balanced Original Features)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves cannot be plotted as there is only one class in the test set.")

# Calculate and display the Confusion Matrix as a heatmap (Training Data)
cm_balanced_orig_train_svm = confusion_matrix(y_train_balanced_orig, svm_preds_balanced_orig_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Training Data):")
display(cm_balanced_orig_train_svm)


# 10. Calculate and print metrics derived from the Confusion Matrix (Training Data)
n_classes_balanced_orig_train_svm = cm_balanced_orig_train_svm.shape[0]
metrics_agg_balanced_orig_train_svm = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_train_svm):
    tp = cm_balanced_orig_train_svm[i, i]
    fn = np.sum(cm_balanced_orig_train_svm[i, :]) - tp
    fp = np.sum(cm_balanced_orig_train_svm[:, i]) - tp
    tn = np.sum(cm_balanced_orig_train_svm) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_train_svm['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_train_svm['specificity'] += specificity_class

sensitivity_macro_balanced_orig_train_svm = metrics_agg_balanced_orig_train_svm['sensitivity'] / n_classes_balanced_orig_train_svm
specificity_macro_balanced_orig_train_svm = metrics_agg_balanced_orig_train_svm['specificity'] / n_classes_balanced_orig_train_svm

type1_error_balanced_orig_train_svm = 1 - specificity_macro_balanced_orig_train_svm
type2_error_balanced_orig_train_svm = 1 - sensitivity_macro_balanced_orig_train_svm
g_mean_balanced_orig_train_svm = np.sqrt(sensitivity_macro_balanced_orig_train_svm * specificity_macro_balanced_orig_train_svm)
youdens_index_balanced_orig_train_svm = sensitivity_macro_balanced_orig_train_svm + specificity_macro_balanced_orig_train_svm - 1

# Avoid division by zero for nlir calculation
nlir_balanced_orig_train_svm = type2_error_balanced_orig_train_svm / specificity_macro_balanced_orig_train_svm if specificity_macro_balanced_orig_train_svm > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_train_svm = np.log((sensitivity_macro_balanced_orig_train_svm / (type1_error_balanced_orig_train_svm if type1_error_balanced_orig_train_svm > 0 else 1e-9)) * (specificity_macro_balanced_orig_train_svm / (type2_error_balanced_orig_train_svm if type2_error_balanced_orig_train_svm > 0 else 1e-9))) if (type1_error_balanced_orig_train_svm > 0 and type2_error_balanced_orig_train_svm > 0 and sensitivity_macro_balanced_orig_train_svm > 0 and specificity_macro_balanced_orig_train_svm > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_train_svm:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_train_svm:.4f}")
print(f"G-mean: {g_mean_balanced_orig_train_svm:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_train_svm:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_train_svm:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_train_svm:.4f}")


# Compute ROC curve and ROC area for each class (Training Data)
# Check if there are at least two classes present in y_train_balanced_orig_binarized to plot ROC curves
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
if y_train_balanced_orig_binarized.shape[1] >= 2:
    fpr_svm_train = dict()
    tpr_svm_train = dict()
    roc_auc_svm_train = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        fpr_svm_train[i], tpr_svm_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], svm_probabilities_balanced_orig_train[:, i])
        roc_auc_svm_train[i] = auc(fpr_svm_train[i], tpr_svm_train[i])

    # Plot the ROC curves (Training Data)
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_svm_train[i], tpr_svm_train[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_svm_train[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - SVM (Balanced Original Features - Training Data)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves for training data cannot be plotted as there is only one class in the balanced training set.")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig, y_original are available

# 1. Initialize a Random Forest Classifier
rf_model_balanced_orig = RandomForestClassifier(n_estimators=100, random_state=42) # Using 100 estimators as a starting point

# 2. Train the Random Forest model on the balanced original training data
print("\nTraining Random Forest model...")
rf_model_balanced_orig.fit(X_train_balanced_orig, y_train_balanced_orig)
print("Random Forest model training complete.")

# 3. Make predictions and get probability estimates on the balanced training data
rf_preds_balanced_orig_train = rf_model_balanced_orig.predict(X_train_balanced_orig)
rf_probabilities_balanced_orig_train = rf_model_balanced_orig.predict_proba(X_train_balanced_orig)


# 4. Make predictions and get probability estimates on the scaled original testing data
rf_preds_balanced_orig_test = rf_model_balanced_orig.predict(X_test_orig_scaled)
rf_probabilities_balanced_orig_test = rf_model_balanced_orig.predict_proba(X_test_orig_scaled)

# Define classes for metrics calculation (using all unique classes from the original data)
# ALL_CLASSES_ORIG is assumed to be defined from previous cells

# 5. Calculate and print performance metrics on Balanced Training Data
print("--- Random Forest Performance Measures (Balanced Original Features - Training Data) ---")

accuracy_train_balanced_orig_rf = accuracy_score(y_train_balanced_orig, rf_preds_balanced_orig_train)
balanced_accuracy_train_balanced_orig_rf = balanced_accuracy_score(y_train_balanced_orig, rf_preds_balanced_orig_train)
precision_train_balanced_orig_rf = precision_score(y_train_balanced_orig, rf_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_rf = recall_score(y_train_balanced_orig, rf_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_rf = f1_score(y_train_balanced_orig, rf_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_rf = cohen_kappa_score(y_train_balanced_orig, rf_preds_balanced_orig_train)
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
auc_train_balanced_orig_rf = roc_auc_score(y_train_balanced_orig_binarized, rf_probabilities_balanced_orig_train, multi_class='ovr', average='macro')


print(f"Accuracy: {accuracy_train_balanced_orig_rf * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_rf * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_rf * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_rf * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_rf * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_rf * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_rf:.4f}")


# 6. Calculate and print performance metrics on Testing Data
print("\n--- Random Forest Performance Measures (Balanced Original Features - Testing Data) ---")

accuracy_test_balanced_orig_rf = accuracy_score(y_test_orig, rf_preds_balanced_orig_test)
balanced_accuracy_test_balanced_orig_rf = balanced_accuracy_score(y_test_orig, rf_preds_balanced_orig_test)
precision_test_balanced_orig_rf = precision_score(y_test_orig, rf_preds_balanced_orig_test, average='macro', zero_division=0)
recall_test_balanced_orig_rf = recall_score(y_test_orig, rf_preds_balanced_orig_test, average='macro', zero_division=0)
f1_test_balanced_orig_rf = f1_score(y_test_orig, rf_preds_balanced_orig_test, average='macro', zero_division=0)
kappa_test_balanced_orig_rf = cohen_kappa_score(y_test_orig, rf_preds_balanced_orig_test)


print(f"Accuracy: {accuracy_test_balanced_orig_rf * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_balanced_orig_rf * 100:.2f}%")
print(f"Precision (Macro): {precision_test_balanced_orig_rf * 100:.2f}%")
print(f"Recall (Macro): {recall_test_balanced_orig_rf * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_balanced_orig_rf * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_balanced_orig_rf:.4f}")

# Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
# Check if there are at least two classes present in y_test_orig_binarized to calculate AUC
if y_test_orig_binarized.shape[1] >= 2:
    auc_test_balanced_orig_rf = roc_auc_score(y_test_orig_binarized, rf_probabilities_balanced_orig_test, multi_class='ovr', average='macro')
    print(f"Area Under the Curve (AUC): {auc_test_balanced_orig_rf * 100:.2f}%")
else:
    print("AUC cannot be calculated as there is only one class in the test set.")


# 7. Calculate and display the Confusion Matrix as a heatmap (Testing Data)
cm_balanced_orig_rf = confusion_matrix(y_test_orig, rf_preds_balanced_orig_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Testing Data):")
display(cm_balanced_orig_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_rf, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Random Forest (Balanced Original Features - Testing Data)')
plt.show()


# 8. Calculate and print metrics derived from the Confusion Matrix (Testing Data)
n_classes_balanced_orig_rf = cm_balanced_orig_rf.shape[0]
metrics_agg_balanced_orig_rf = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_rf):
    tp = cm_balanced_orig_rf[i, i]
    fn = np.sum(cm_balanced_orig_rf[i, :]) - tp
    fp = np.sum(cm_balanced_orig_rf[:, i]) - tp
    tn = np.sum(cm_balanced_orig_rf) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_rf['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_rf['specificity'] += specificity_class

sensitivity_macro_balanced_orig_rf = metrics_agg_balanced_orig_rf['sensitivity'] / n_classes_balanced_orig_rf
specificity_macro_balanced_orig_rf = metrics_agg_balanced_orig_rf['specificity'] / n_classes_balanced_orig_rf

type1_error_balanced_orig_rf = 1 - specificity_macro_balanced_orig_rf
type2_error_balanced_orig_rf = 1 - sensitivity_macro_balanced_orig_rf
g_mean_balanced_orig_rf = np.sqrt(sensitivity_macro_balanced_orig_rf * specificity_macro_balanced_orig_rf)
youdens_index_balanced_orig_rf = sensitivity_macro_balanced_orig_rf + specificity_macro_balanced_orig_rf - 1
# Avoid division by zero for nlir calculation
nlir_balanced_orig_rf = type2_error_balanced_orig_rf / specificity_macro_balanced_orig_rf if specificity_macro_balanced_orig_rf > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_rf = np.log((sensitivity_macro_balanced_orig_rf / (type1_error_balanced_orig_rf if type1_error_balanced_orig_rf > 0 else 1e-9)) * (specificity_macro_balanced_orig_rf / (type2_error_balanced_orig_rf if type2_error_balanced_orig_rf > 0 else 1e-9))) if (type1_error_balanced_orig_rf > 0 and type2_error_balanced_orig_rf > 0 and sensitivity_macro_balanced_orig_rf > 0 and specificity_macro_balanced_orig_rf > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_rf:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_rf:.4f}")
print(f"G-mean: {g_mean_balanced_orig_rf:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_rf:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_rf:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_rf:.4f}")


# 9. Compute and plot the ROC curves for each class (Testing Data)
# Check if there are at least two classes present in y_test_orig_binarized to plot ROC curves
if y_test_orig_binarized.shape[1] >= 2:
    fpr_rf = dict()
    tpr_rf = dict()
    roc_auc_rf = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        # roc_curve expects binary labels, so we use the binarized y_test
        fpr_rf[i], tpr_rf[i], _ = roc_curve(y_test_orig_binarized[:, i], rf_probabilities_balanced_orig_test[:, i])
        roc_auc_rf[i] = auc(fpr_rf[i], tpr_rf[i])

    # Plot the ROC curves
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_rf[i], tpr_rf[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_rf[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest (Balanced Original Features)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves cannot be plotted as there is only one class in the test set.")

# Calculate and display the Confusion Matrix as a heatmap (Training Data)
cm_balanced_orig_train_rf = confusion_matrix(y_train_balanced_orig, rf_preds_balanced_orig_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Training Data):")
display(cm_balanced_orig_train_rf)


# 10. Calculate and print metrics derived from the Confusion Matrix (Training Data)
n_classes_balanced_orig_train_rf = cm_balanced_orig_train_rf.shape[0]
metrics_agg_balanced_orig_train_rf = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_train_rf):
    tp = cm_balanced_orig_train_rf[i, i]
    fn = np.sum(cm_balanced_orig_train_rf[i, :]) - tp
    fp = np.sum(cm_balanced_orig_train_rf[:, i]) - tp
    tn = np.sum(cm_balanced_orig_train_rf) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_train_rf['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_train_rf['specificity'] += specificity_class

sensitivity_macro_balanced_orig_train_rf = metrics_agg_balanced_orig_train_rf['sensitivity'] / n_classes_balanced_orig_train_rf
specificity_macro_balanced_orig_train_rf = metrics_agg_balanced_orig_train_rf['specificity'] / n_classes_balanced_orig_train_rf

type1_error_balanced_orig_train_rf = 1 - specificity_macro_balanced_orig_train_rf
type2_error_balanced_orig_train_rf = 1 - sensitivity_macro_balanced_orig_train_rf
g_mean_balanced_orig_train_rf = np.sqrt(sensitivity_macro_balanced_orig_train_rf * specificity_macro_balanced_orig_train_rf)
youdens_index_balanced_orig_train_rf = sensitivity_macro_balanced_orig_train_rf + specificity_macro_balanced_orig_train_rf - 1

# Avoid division by zero for nlir calculation
nlir_balanced_orig_train_rf = type2_error_balanced_orig_train_rf / specificity_macro_balanced_orig_train_rf if specificity_macro_balanced_orig_train_rf > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_train_rf = np.log((sensitivity_macro_balanced_orig_train_rf / (type1_error_balanced_orig_train_rf if type1_error_balanced_orig_train_rf > 0 else 1e-9)) * (specificity_macro_balanced_orig_train_rf / (type2_error_balanced_orig_train_rf if type2_error_balanced_orig_train_rf > 0 else 1e-9))) if (type1_error_balanced_orig_train_rf > 0 and type2_error_balanced_orig_train_rf > 0 and sensitivity_macro_balanced_orig_train_rf > 0 and specificity_macro_balanced_orig_train_rf > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_train_rf:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_train_rf:.4f}")
print(f"G-mean: {g_mean_balanced_orig_train_rf:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_train_rf:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_train_rf:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_train_rf:.4f}")


# Compute ROC curve and ROC area for each class (Training Data)
# Check if there are at least two classes present in y_train_balanced_orig_binarized to plot ROC curves
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
if y_train_balanced_orig_binarized.shape[1] >= 2:
    fpr_rf_train = dict()
    tpr_rf_train = dict()
    roc_auc_rf_train = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        fpr_rf_train[i], tpr_rf_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], rf_probabilities_balanced_orig_train[:, i])
        roc_auc_rf_train[i] = auc(fpr_rf_train[i], tpr_rf_train[i])

    # Plot the ROC curves (Training Data)
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_rf_train[i], tpr_rf_train[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_rf_train[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest (Balanced Original Features - Training Data)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves for training data cannot be plotted as there is only one class in the balanced training set.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score
from sklearn.preprocessing import label_binarize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig are available from previous cell

# 1. Convert target variables to start from 0 for MLP
y_train_balanced_orig_mlp = y_train_balanced_orig - 1
y_test_orig_mlp = y_test_orig - 1

# Define classes for metrics calculation (using all unique classes from the original data)
ALL_CLASSES_ORIG = sorted(y_original.unique())


# 2. Initialize the Sequential MLP model
mlp_model_balanced_orig = Sequential([
    Dense(512, activation='relu', input_shape=(X_train_balanced_orig.shape[1],)),
    Dense(250, activation='relu'),
    Dense(120, activation='relu'),
    Dense(80, activation='relu'),
    Dense(60, activation='relu'),
    Dense(len(ALL_CLASSES_ORIG), activation='softmax') # Number of units equals the number of unique classes
])

# 3. Compile the MLP model
mlp_model_balanced_orig.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 4. Train the MLP model on balanced training data
print("\nTraining MLP model...")
history = mlp_model_balanced_orig.fit(X_train_balanced_orig, y_train_balanced_orig_mlp, epochs=30, verbose=0)
print("MLP model training complete.")

# 5. Evaluate MLP model on balanced training data
print("--- MLP Performance Measures (Balanced Original Features - Training Data) ---")
_, mlp_accuracy_balanced_orig_train = mlp_model_balanced_orig.evaluate(X_train_balanced_orig, y_train_balanced_orig_mlp, verbose=0)
mlp_probabilities_balanced_orig_train = mlp_model_balanced_orig.predict(X_train_balanced_orig)
mlp_preds_balanced_orig_train_raw = np.argmax(mlp_probabilities_balanced_orig_train, axis=1)
mlp_preds_balanced_orig_train = mlp_preds_balanced_orig_train_raw + 1


balanced_accuracy_train_balanced_orig_mlp = balanced_accuracy_score(y_train_balanced_orig, mlp_preds_balanced_orig_train)
precision_train_balanced_orig_mlp = precision_score(y_train_balanced_orig, mlp_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_mlp = recall_score(y_train_balanced_orig, mlp_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_mlp = f1_score(y_train_balanced_orig, mlp_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_mlp = cohen_kappa_score(y_train_balanced_orig, mlp_preds_balanced_orig_train)
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)
auc_train_balanced_orig_mlp = roc_auc_score(y_train_balanced_orig_binarized, mlp_probabilities_balanced_orig_train, multi_class='ovr', average='macro')


print(f"Accuracy: {mlp_accuracy_balanced_orig_train * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_mlp * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_mlp * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_mlp * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_mlp * 100:.2f}%")
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_mlp * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_mlp:.4f}")


# 6. Evaluate MLP model on scaled original testing data
print("\n--- MLP Performance Measures (Balanced Original Features - Testing Data) ---")
_, mlp_accuracy_balanced_orig_test = mlp_model_balanced_orig.evaluate(X_test_orig_scaled, y_test_orig_mlp, verbose=0)
mlp_probabilities_balanced_orig_test = mlp_model_balanced_orig.predict(X_test_orig_scaled)
mlp_predictions_raw_balanced_orig_test = np.argmax(mlp_probabilities_balanced_orig_test, axis=1)
mlp_preds_balanced_orig_test = mlp_predictions_raw_balanced_orig_test + 1


balanced_accuracy_test_balanced_orig_mlp = balanced_accuracy_score(y_test_orig, mlp_preds_balanced_orig_test)
precision_test_balanced_orig_mlp = precision_score(y_test_orig, mlp_preds_balanced_orig_test, average='macro', zero_division=0)
recall_test_balanced_orig_mlp = recall_score(y_test_orig, mlp_preds_balanced_orig_test, average='macro', zero_division=0)
f1_test_balanced_orig_mlp = f1_score(y_test_orig, mlp_preds_balanced_orig_test, average='macro', zero_division=0)
kappa_test_balanced_orig_mlp = cohen_kappa_score(y_test_orig, mlp_preds_balanced_orig_test)


print(f"Accuracy: {mlp_accuracy_balanced_orig_test * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_balanced_orig_mlp * 100:.2f}%")
print(f"Precision (Macro): {precision_test_balanced_orig_mlp * 100:.2f}%")
print(f"Recall (Macro): {recall_test_balanced_orig_mlp * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_balanced_orig_mlp * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_balanced_orig_mlp:.4f}")


# 7. Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# 8. Calculate and print AUC score
auc_test_balanced_orig_mlp = roc_auc_score(y_test_orig_binarized, mlp_probabilities_balanced_orig_test, multi_class='ovr', average='macro')
print(f"Area Under the Curve (AUC): {auc_test_balanced_orig_mlp * 100:.2f}%")

# 9. Calculate Confusion Matrix
cm_balanced_orig_mlp = confusion_matrix(y_test_orig, mlp_preds_balanced_orig_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Testing Data):")
display(cm_balanced_orig_mlp)

# 10. Generate Confusion Matrix Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_mlp, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - MLP (Balanced Original Features - Testing Data)')
plt.show()

# 11. Calculate and print metrics from Confusion Matrix
n_classes_balanced_orig_mlp = cm_balanced_orig_mlp.shape[0]
metrics_agg_balanced_orig_mlp = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_mlp):
    tp = cm_balanced_orig_mlp[i, i]
    fn = np.sum(cm_balanced_orig_mlp[i, :]) - tp
    fp = np.sum(cm_balanced_orig_mlp[:, i]) - tp
    tn = np.sum(cm_balanced_orig_mlp) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_mlp['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_mlp['specificity'] += specificity_class

sensitivity_macro_balanced_orig_mlp = metrics_agg_balanced_orig_mlp['sensitivity'] / n_classes_balanced_orig_mlp
specificity_macro_balanced_orig_mlp = metrics_agg_balanced_orig_mlp['specificity'] / n_classes_balanced_orig_mlp

type1_error_balanced_orig_mlp = 1 - specificity_macro_balanced_orig_mlp
type2_error_balanced_orig_mlp = 1 - sensitivity_macro_balanced_orig_mlp
g_mean_balanced_orig_mlp = np.sqrt(sensitivity_macro_balanced_orig_mlp * specificity_macro_balanced_orig_mlp)
youdens_index_balanced_orig_mlp = sensitivity_macro_balanced_orig_mlp + specificity_macro_balanced_orig_mlp - 1
# Avoid division by zero for nlir calculation
nlir_balanced_orig_mlp = type2_error_balanced_orig_mlp / specificity_macro_balanced_orig_mlp if specificity_macro_balanced_orig_mlp > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_mlp = np.log((sensitivity_macro_balanced_orig_mlp / (type1_error_balanced_orig_mlp if type1_error_balanced_orig_mlp > 0 else 1e-9)) * (specificity_macro_balanced_orig_mlp / (type2_error_balanced_orig_mlp if type2_error_balanced_orig_mlp > 0 else 1e-9))) if (type1_error_balanced_orig_mlp > 0 and type2_error_balanced_orig_mlp > 0 and sensitivity_macro_balanced_orig_mlp > 0 and specificity_macro_balanced_orig_mlp > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_mlp:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_mlp:.4f}")
print(f"G-mean: {g_mean_balanced_orig_mlp:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_mlp:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_mlp:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_mlp:.4f}")

# Compute ROC curve and ROC area for each class on testing data
fpr_mlp = dict()
tpr_mlp = dict()
roc_auc_mlp = dict()
for i in range(len(ALL_CLASSES_ORIG)):
    # roc_curve expects binary labels, so we use the binarized y_test
    fpr_mlp[i], tpr_mlp[i], _ = roc_curve(y_test_orig_binarized[:, i], mlp_probabilities_balanced_orig_test[:, i])
    roc_auc_mlp[i] = auc(fpr_mlp[i], tpr_mlp[i])

# Plot the ROC curves for testing data
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
    plt.plot(fpr_mlp[i], tpr_mlp[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_mlp[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP (Balanced Original Features - Testing Data)')
plt.legend(loc="lower right")
plt.show()


# Evaluate MLP model on balanced training data for additional metrics and ROC
print("\n--- MLP Performance Measures (Balanced Original Features - Training Data) ---")
mlp_probabilities_balanced_orig_train = mlp_model_balanced_orig.predict(X_train_balanced_orig)
mlp_preds_balanced_orig_train_raw = np.argmax(mlp_probabilities_balanced_orig_train, axis=1)
mlp_preds_balanced_orig_train = mlp_preds_balanced_orig_train_raw + 1

balanced_accuracy_train_balanced_orig_mlp = balanced_accuracy_score(y_train_balanced_orig, mlp_preds_balanced_orig_train)
precision_train_balanced_orig_mlp = precision_score(y_train_balanced_orig, mlp_preds_balanced_orig_train, average='macro', zero_division=0)
recall_train_balanced_orig_mlp = recall_score(y_train_balanced_orig, mlp_preds_balanced_orig_train, average='macro', zero_division=0)
f1_train_balanced_orig_mlp = f1_score(y_train_balanced_orig, mlp_preds_balanced_orig_train, average='macro', zero_division=0)
kappa_train_balanced_orig_mlp = cohen_kappa_score(y_train_balanced_orig, mlp_preds_balanced_orig_train)

print(f"Balanced Accuracy: {balanced_accuracy_train_balanced_orig_mlp * 100:.2f}%")
print(f"Precision (Macro): {precision_train_balanced_orig_mlp * 100:.2f}%")
print(f"Recall (Macro): {recall_train_balanced_orig_mlp * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_balanced_orig_mlp * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_balanced_orig_mlp:.4f}")


# Binarize the balanced training target variable for AUC
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score for training data
auc_train_balanced_orig_mlp = roc_auc_score(y_train_balanced_orig_binarized, mlp_probabilities_balanced_orig_train, multi_class='ovr', average='macro')
print(f"Area Under the Curve (AUC): {auc_train_balanced_orig_mlp * 100:.2f}%")

# Confusion Matrix for training data
cm_balanced_orig_train_mlp = confusion_matrix(y_train_balanced_orig, mlp_preds_balanced_orig_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (Balanced Original Features - Training Data):")
display(cm_balanced_orig_train_mlp)

# Generate Confusion Matrix Heatmap for training data
plt.figure(figsize=(8, 6))
sns.heatmap(cm_balanced_orig_train_mlp, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - MLP (Balanced Original Features - Training Data)')
plt.show()

# Calculate and print metrics from training Confusion Matrix
n_classes_balanced_orig_train_mlp = cm_balanced_orig_train_mlp.shape[0]
metrics_agg_balanced_orig_train_mlp = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_balanced_orig_train_mlp):
    tp = cm_balanced_orig_train_mlp[i, i]
    fn = np.sum(cm_balanced_orig_train_mlp[i, :]) - tp
    fp = np.sum(cm_balanced_orig_train_mlp[:, i]) - tp
    tn = np.sum(cm_balanced_orig_train_mlp) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_balanced_orig_train_mlp['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_balanced_orig_train_mlp['specificity'] += specificity_class

sensitivity_macro_balanced_orig_train_mlp = metrics_agg_balanced_orig_train_mlp['sensitivity'] / n_classes_balanced_orig_train_mlp
specificity_macro_balanced_orig_train_mlp = metrics_agg_balanced_orig_train_mlp['specificity'] / n_classes_balanced_orig_train_mlp

type1_error_balanced_orig_train_mlp = 1 - specificity_macro_balanced_orig_train_mlp
type2_error_balanced_orig_train_mlp = 1 - sensitivity_macro_balanced_orig_train_mlp
g_mean_balanced_orig_train_mlp = np.sqrt(sensitivity_macro_balanced_orig_train_mlp * specificity_macro_balanced_orig_train_mlp)
youdens_index_balanced_orig_train_mlp = sensitivity_macro_balanced_orig_train_mlp + specificity_macro_balanced_orig_train_mlp - 1

# Avoid division by zero for nlir calculation
nlir_balanced_orig_train_mlp = type2_error_balanced_orig_train_mlp / specificity_macro_balanced_orig_train_mlp if specificity_macro_balanced_orig_train_mlp > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_balanced_orig_train_mlp = np.log((sensitivity_macro_balanced_orig_train_mlp / (type1_error_balanced_orig_train_mlp if type1_error_balanced_orig_train_mlp > 0 else 1e-9)) * (specificity_macro_balanced_orig_train_mlp / (type2_error_balanced_orig_train_mlp if type2_error_balanced_orig_train_mlp > 0 else 1e-9))) if (type1_error_balanced_orig_train_mlp > 0 and type2_error_balanced_orig_train_mlp > 0 and sensitivity_macro_balanced_orig_train_mlp > 0 and specificity_macro_balanced_orig_train_mlp > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_balanced_orig_train_mlp:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_balanced_orig_train_mlp:.4f}")
print(f"G-mean: {g_mean_balanced_orig_train_mlp:.4f}")
print(f"Youden's Index: {youdens_index_balanced_orig_train_mlp:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_balanced_orig_train_mlp:.4f}")
print(f"Discriminant Power: {discriminant_power_balanced_orig_train_mlp:.4f}")


# Compute ROC curve and ROC area for each class on training data
fpr_mlp_train = dict()
tpr_mlp_train = dict()
roc_auc_mlp_train = dict()
for i in range(len(ALL_CLASSES_ORIG)):
    fpr_mlp_train[i], tpr_mlp_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], mlp_probabilities_balanced_orig_train[:, i])
    roc_auc_mlp_train[i] = auc(fpr_mlp_train[i], tpr_mlp_train[i])

# Plot the ROC curves for training data
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
    plt.plot(fpr_mlp_train[i], tpr_mlp_train[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_mlp_train[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP (Balanced Original Features - Training Data)')
plt.legend(loc="lower right")
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input, concatenate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, balanced_accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train_balanced_orig, y_train_balanced_orig, X_test_orig_scaled, y_test_orig, y_original are available

# 1. Train the Random Forest model (as a component of the hybrid)
# We'll use the same RF model trained in a previous step if available, otherwise train a new one.
# For simplicity, let's re-initialize and train the RF model on the balanced data
print("\nTraining Random Forest component for hybrid model...")
rf_component = RandomForestClassifier(n_estimators=100, random_state=42)
rf_component.fit(X_train_balanced_orig, y_train_balanced_orig)
print("Random Forest component training complete.")

# 2. Get predictions (probabilities) from the Random Forest component
# These predictions will be used as features for the MLP component
rf_train_probs = rf_component.predict_proba(X_train_balanced_orig)
rf_test_probs = rf_component.predict_proba(X_test_orig_scaled)

# 3. Define the MLP model structure
# This MLP will take the original features PLUS the RF probabilities as input
print("\nDefining MLP component for hybrid model...")
input_shape_mlp = X_train_balanced_orig.shape[1] + rf_train_probs.shape[1] # Original features + RF probabilities
num_classes_orig = len(ALL_CLASSES_ORIG) # Ensure this is defined

mlp_component_input = Input(shape=(input_shape_mlp,))
x = Dense(512, activation='relu')(mlp_component_input)
x = Dense(250, activation='relu')(x)
x = Dense(120, activation='relu')(x)
x = Dense(80, activation='relu')(x)
x = Dense(60, activation='relu')(x)
mlp_output = Dense(num_classes_orig, activation='softmax')(x) # Output layer matches number of classes

mlp_component_model = Model(inputs=mlp_component_input, outputs=mlp_output)

mlp_component_model.compile(optimizer='adam',
                            loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer labels
                            metrics=['accuracy'])
print("MLP component defined.")

# 4. Prepare combined features for the MLP component
X_train_combined = np.concatenate((X_train_balanced_orig, rf_train_probs), axis=1)
X_test_combined = np.concatenate((X_test_orig_scaled, rf_test_probs), axis=1)

# Adjust target labels to be 0-indexed for sparse_categorical_crossentropy
y_train_balanced_orig_hybrid_mlp = y_train_balanced_orig - 1
y_test_orig_hybrid_mlp = y_test_orig - 1


# 5. Train the MLP component of the hybrid model on the combined features
print("\nTraining MLP component with combined features...")
history_hybrid_mlp = mlp_component_model.fit(X_train_combined, y_train_balanced_orig_hybrid_mlp,
                                             epochs=50, # You can adjust the number of epochs
                                             batch_size=32, # You can adjust the batch size
                                             verbose=0) # Set to 1 or 2 for progress bars
print("MLP component training complete.")

# 6. Evaluate the Hybrid Model on Testing Data
print("\n--- RF-MLP Hybrid Model Performance Measures (Balanced Original Features - Testing Data) ---")

# Get probability estimates and predictions from the hybrid model
hybrid_probabilities_test = mlp_component_model.predict(X_test_combined)
hybrid_preds_test_raw = np.argmax(hybrid_probabilities_test, axis=1)
hybrid_preds_test = hybrid_preds_test_raw + 1 # Convert back to original class labels

# Calculate and print performance metrics on Testing Data
accuracy_test_hybrid = accuracy_score(y_test_orig, hybrid_preds_test)
balanced_accuracy_test_hybrid = balanced_accuracy_score(y_test_orig, hybrid_preds_test)
precision_test_hybrid = precision_score(y_test_orig, hybrid_preds_test, average='macro', zero_division=0)
recall_test_hybrid = recall_score(y_test_orig, hybrid_preds_test, average='macro', zero_division=0)
f1_test_hybrid = f1_score(y_test_orig, hybrid_preds_test, average='macro', zero_division=0)
kappa_test_hybrid = cohen_kappa_score(y_test_orig, hybrid_preds_test)

print(f"Accuracy: {accuracy_test_hybrid * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_test_hybrid * 100:.2f}%")
print(f"Precision (Macro): {precision_test_hybrid * 100:.2f}%")
print(f"Recall (Macro): {recall_test_hybrid * 100:.2f}%")
print(f"F1 Score (Macro): {f1_test_hybrid * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_test_hybrid:.4f}")

# Binarize the original testing target variable for AUC
y_test_orig_binarized = label_binarize(y_test_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
# Check if there are at least two classes present in y_test_orig_binarized to calculate AUC
if y_test_orig_binarized.shape[1] >= 2:
    auc_test_hybrid = roc_auc_score(y_test_orig_binarized, hybrid_probabilities_test, multi_class='ovr', average='macro')
    print(f"Area Under the Curve (AUC): {auc_test_hybrid * 100:.2f}%")
else:
    print("AUC cannot be calculated as there is only one class in the test set.")


# 7. Calculate and display the Confusion Matrix as a heatmap (Testing Data)
cm_hybrid_test = confusion_matrix(y_test_orig, hybrid_preds_test, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (RF-MLP Hybrid - Testing Data):")
display(cm_hybrid_test)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_hybrid_test, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - RF-MLP Hybrid (Balanced Original Features - Testing Data)')
plt.show()

# 8. Calculate and print metrics derived from the Confusion Matrix (Testing Data)
n_classes_hybrid_test = cm_hybrid_test.shape[0]
metrics_agg_hybrid_test = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_hybrid_test):
    tp = cm_hybrid_test[i, i]
    fn = np.sum(cm_hybrid_test[i, :]) - tp
    fp = np.sum(cm_hybrid_test[:, i]) - tp
    tn = np.sum(cm_hybrid_test) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_hybrid_test['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_hybrid_test['specificity'] += specificity_class

sensitivity_macro_hybrid_test = metrics_agg_hybrid_test['sensitivity'] / n_classes_hybrid_test
specificity_macro_hybrid_test = metrics_agg_hybrid_test['specificity'] / n_classes_hybrid_test

type1_error_hybrid_test = 1 - specificity_macro_hybrid_test
type2_error_hybrid_test = 1 - sensitivity_macro_hybrid_test
g_mean_hybrid_test = np.sqrt(sensitivity_macro_hybrid_test * specificity_macro_hybrid_test)
youdens_index_hybrid_test = sensitivity_macro_hybrid_test + specificity_macro_hybrid_test - 1
# Avoid division by zero for nlir calculation
nlir_hybrid_test = type2_error_hybrid_test / specificity_macro_hybrid_test if specificity_macro_hybrid_test > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_hybrid_test = np.log((sensitivity_macro_hybrid_test / (type1_error_hybrid_test if type1_error_hybrid_test > 0 else 1e-9)) * (specificity_macro_hybrid_test / (type2_error_hybrid_test if type2_error_hybrid_test > 0 else 1e-9))) if (type1_error_hybrid_test > 0 and type2_error_hybrid_test > 0 and sensitivity_macro_hybrid_test > 0 and specificity_macro_hybrid_test > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_hybrid_test:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_hybrid_test:.4f}")
print(f"G-mean: {g_mean_hybrid_test:.4f}")
print(f"Youden's Index: {youdens_index_hybrid_test:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_hybrid_test:.4f}")
print(f"Discriminant Power: {discriminant_power_hybrid_test:.4f}")

# 9. Compute and plot the ROC curves for each class (Testing Data)
# Check if there are at least two classes present in y_test_orig_binarized to plot ROC curves
if y_test_orig_binarized.shape[1] >= 2:
    fpr_hybrid = dict()
    tpr_hybrid = dict()
    roc_auc_hybrid = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        # roc_curve expects binary labels, so we use the binarized y_test
        fpr_hybrid[i], tpr_hybrid[i], _ = roc_curve(y_test_orig_binarized[:, i], hybrid_probabilities_test[:, i])
        roc_auc_hybrid[i] = auc(fpr_hybrid[i], tpr_hybrid[i])

    # Plot the ROC curves
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'] # Define colors for each class
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_hybrid[i], tpr_hybrid[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_hybrid[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Plot the diagonal random guess line
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - RF-MLP Hybrid (Balanced Original Features)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves cannot be plotted as there is only one class in the test set.")

# Evaluate the Hybrid Model on Training Data
print("\n--- RF-MLP Hybrid Model Performance Measures (Balanced Original Features - Training Data) ---")

# Get probability estimates and predictions from the hybrid model
hybrid_probabilities_train = mlp_component_model.predict(X_train_combined)
hybrid_preds_train_raw = np.argmax(hybrid_probabilities_train, axis=1)
hybrid_preds_train = hybrid_preds_train_raw + 1 # Convert back to original class labels

# Calculate and print performance metrics on Training Data
accuracy_train_hybrid = accuracy_score(y_train_balanced_orig, hybrid_preds_train)
balanced_accuracy_train_hybrid = balanced_accuracy_score(y_train_balanced_orig, hybrid_preds_train)
precision_train_hybrid = precision_score(y_train_balanced_orig, hybrid_preds_train, average='macro', zero_division=0)
recall_train_hybrid = recall_score(y_train_balanced_orig, hybrid_preds_train, average='macro', zero_division=0)
f1_train_hybrid = f1_score(y_train_balanced_orig, hybrid_preds_train, average='macro', zero_division=0)
kappa_train_hybrid = cohen_kappa_score(y_train_balanced_orig, hybrid_preds_train)

print(f"Accuracy: {accuracy_train_hybrid * 100:.2f}%")
print(f"Balanced Accuracy: {balanced_accuracy_train_hybrid * 100:.2f}%")
print(f"Precision (Macro): {precision_train_hybrid * 100:.2f}%")
print(f"Recall (Macro): {recall_train_hybrid * 100:.2f}%")
print(f"F1 Score (Macro): {f1_train_hybrid * 100:.2f}%")
print(f"Cohen's Kappa: {kappa_train_hybrid:.4f}")

# Binarize the balanced training target variable for AUC
y_train_balanced_orig_binarized = label_binarize(y_train_balanced_orig, classes=ALL_CLASSES_ORIG)

# Calculate and print AUC score
# Check if there are at least two classes present in y_train_balanced_orig_binarized to calculate AUC
if y_train_balanced_orig_binarized.shape[1] >= 2:
    auc_train_hybrid = roc_auc_score(y_train_balanced_orig_binarized, hybrid_probabilities_train, multi_class='ovr', average='macro')
    print(f"Area Under the Curve (AUC): {auc_train_hybrid * 100:.2f}%")
else:
    print("AUC cannot be calculated as there is only one class in the balanced training set.")

# Calculate and display the Confusion Matrix as a heatmap (Training Data)
cm_hybrid_train = confusion_matrix(y_train_balanced_orig, hybrid_preds_train, labels=ALL_CLASSES_ORIG)
print("\nConfusion Matrix (RF-MLP Hybrid - Training Data):")
display(cm_hybrid_train)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_hybrid_train, annot=True, fmt='d', cmap='Blues', xticklabels=ALL_CLASSES_ORIG, yticklabels=ALL_CLASSES_ORIG)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - RF-MLP Hybrid (Balanced Original Features - Training Data)')
plt.show()

# Calculate and print metrics derived from the Confusion Matrix (Training Data)
n_classes_hybrid_train = cm_hybrid_train.shape[0]
metrics_agg_hybrid_train = {'sensitivity': 0, 'specificity': 0}

for i in range(n_classes_hybrid_train):
    tp = cm_hybrid_train[i, i]
    fn = np.sum(cm_hybrid_train[i, :]) - tp
    fp = np.sum(cm_hybrid_train[:, i]) - tp
    tn = np.sum(cm_hybrid_train) - (tp + fp + fn)

    sensitivity_class = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics_agg_hybrid_train['sensitivity'] += sensitivity_class

    specificity_class = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics_agg_hybrid_train['specificity'] += specificity_class

sensitivity_macro_hybrid_train = metrics_agg_hybrid_train['sensitivity'] / n_classes_hybrid_train
specificity_macro_hybrid_train = metrics_agg_hybrid_train['specificity'] / n_classes_hybrid_train

type1_error_hybrid_train = 1 - specificity_macro_hybrid_train
type2_error_hybrid_train = 1 - sensitivity_macro_hybrid_train
g_mean_hybrid_train = np.sqrt(sensitivity_macro_hybrid_train * specificity_macro_hybrid_train)
youdens_index_hybrid_train = sensitivity_macro_hybrid_train + specificity_macro_hybrid_train - 1
# Avoid division by zero for nlir calculation
nlir_hybrid_train = type2_error_hybrid_train / specificity_macro_hybrid_train if specificity_macro_hybrid_train > 0 else np.inf # Use infinity for division by zero
# Avoid log of zero or negative numbers for discriminant power
discriminant_power_hybrid_train = np.log((sensitivity_macro_hybrid_train / (type1_error_hybrid_train if type1_error_hybrid_train > 0 else 1e-9)) * (specificity_macro_hybrid_train / (type2_error_hybrid_train if type2_error_hybrid_train > 0 else 1e-9))) if (type1_error_hybrid_train > 0 and type2_error_hybrid_train > 0 and sensitivity_macro_hybrid_train > 0 and specificity_macro_hybrid_train > 0) else np.nan # Use NaN if calculation is invalid


print(f"\nType I Error (False Positive Rate): {type1_error_hybrid_train:.4f}")
print(f"Type II Error (False Negative Rate): {type2_error_hybrid_train:.4f}")
print(f"G-mean: {g_mean_hybrid_train:.4f}")
print(f"Youden's Index: {youdens_index_hybrid_train:.4f}")
print(f"Negative Likelihood Ratio (NLiR): {nlir_hybrid_train:.4f}")
print(f"Discriminant Power: {discriminant_power_hybrid_train:.4f}")

# 10. Compute and plot the ROC curves for each class (Training Data)
# Check if there are at least two classes present in y_train_balanced_orig_binarized to plot ROC curves
if y_train_balanced_orig_binarized.shape[1] >= 2:
    fpr_hybrid_train = dict()
    tpr_hybrid_train = dict()
    roc_auc_hybrid_train = dict()
    for i in range(len(ALL_CLASSES_ORIG)):
        fpr_hybrid_train[i], tpr_hybrid_train[i], _ = roc_curve(y_train_balanced_orig_binarized[:, i], hybrid_probabilities_train[:, i])
        roc_auc_hybrid_train[i] = auc(fpr_hybrid_train[i], tpr_hybrid_train[i])

    # Plot the ROC curves (Training Data)
    plt.figure(figsize=(10, 8))
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
    for i, color in zip(range(len(ALL_CLASSES_ORIG)), colors):
        plt.plot(fpr_hybrid_train[i], tpr_hybrid_train[i], color=color, lw=2,
                 label='ROC curve of class {0} (area = {1:0.2f})'.format(ALL_CLASSES_ORIG[i], roc_auc_hybrid_train[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve - RF-MLP Hybrid (Balanced Original Features - Training Data)')
    plt.legend(loc="lower right")
    plt.show()
else:
    print("\nROC curves for training data cannot be plotted as there is only one class in the balanced training set.")

import pandas as pd

# Initialize dictionaries to store performance metrics for each model on training and testing data
training_metrics = {}
testing_metrics = {}

# --- Collect Metrics for Logistic Regression ---
training_metrics['Logistic Regression'] = {
    'Accuracy': accuracy_train_balanced_orig_lr,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_lr,
    'Precision (Macro)': precision_train_balanced_orig_lr,
    'Recall (Macro)': recall_train_balanced_orig_lr,
    'F1 Score (Macro)': f1_train_balanced_orig_lr,
    'AUC': auc_train_balanced_orig_lr,
    'Cohen\'s Kappa': kappa_train_balanced_orig_lr,
    'Type I Error': type1_error_balanced_orig_train_lr,
    'Type II Error': type2_error_balanced_orig_train_lr,
    'G-mean': g_mean_balanced_orig_train_lr,
    'Youden\'s Index': youdens_index_balanced_orig_train_lr,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_lr,
    'Discriminant Power': discriminant_power_balanced_orig_train_lr
}

testing_metrics['Logistic Regression'] = {
    'Accuracy': accuracy_test_balanced_orig_lr,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_lr,
    'Precision (Macro)': precision_test_balanced_orig_lr,
    'Recall (Macro)': recall_test_balanced_orig_lr,
    'F1 Score (Macro)': f1_test_balanced_orig_lr,
    'AUC': auc_test_balanced_orig_lr,
    'Cohen\'s Kappa': kappa_test_balanced_orig_lr,
    'Type I Error': type1_error_balanced_orig_lr,
    'Type II Error': type2_error_balanced_orig_lr,
    'G-mean': g_mean_balanced_orig_lr,
    'Youden\'s Index': youdens_index_balanced_orig_lr,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_lr,
    'Discriminant Power': discriminant_power_balanced_orig_lr
}

# --- Collect Metrics for Decision Tree ---
training_metrics['Decision Tree'] = {
    'Accuracy': accuracy_train_balanced_orig_dt,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_dt,
    'Precision (Macro)': precision_train_balanced_orig_dt,
    'Recall (Macro)': recall_train_balanced_orig_dt,
    'F1 Score (Macro)': f1_train_balanced_orig_dt,
    'AUC': auc_train_balanced_orig_dt,
    'Cohen\'s Kappa': kappa_train_balanced_orig_dt,
    'Type I Error': type1_error_balanced_orig_train_dt,
    'Type II Error': type2_error_balanced_orig_train_dt,
    'G-mean': g_mean_balanced_orig_train_dt,
    'Youden\'s Index': youdens_index_balanced_orig_train_dt,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_dt,
    'Discriminant Power': discriminant_power_balanced_orig_train_dt
}

testing_metrics['Decision Tree'] = {
    'Accuracy': accuracy_test_balanced_orig_dt,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_dt,
    'Precision (Macro)': precision_test_balanced_orig_dt,
    'Recall (Macro)': recall_test_balanced_orig_dt,
    'F1 Score (Macro)': f1_test_balanced_orig_dt,
    'AUC': auc_test_balanced_orig_dt, # Assuming AUC was calculated for testing data
    'Cohen\'s Kappa': kappa_test_balanced_orig_dt,
    'Type I Error': type1_error_balanced_orig_dt,
    'Type II Error': type2_error_balanced_orig_dt,
    'G-mean': g_mean_balanced_orig_dt,
    'Youden\'s Index': youdens_index_balanced_orig_dt,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_dt,
    'Discriminant Power': discriminant_power_balanced_orig_dt
}


# --- Collect Metrics for XGBoost ---
training_metrics['XGBoost'] = {
    'Accuracy': accuracy_train_balanced_orig_xgb,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_xgb,
    'Precision (Macro)': precision_train_balanced_orig_xgb,
    'Recall (Macro)': recall_train_balanced_orig_xgb,
    'F1 Score (Macro)': f1_train_balanced_orig_xgb,
    'AUC': auc_train_balanced_orig_xgb,
    'Cohen\'s Kappa': kappa_train_balanced_orig_xgb,
    'Type I Error': type1_error_balanced_orig_train_xgb,
    'Type II Error': type2_error_balanced_orig_train_xgb,
    'G-mean': g_mean_balanced_orig_train_xgb,
    'Youden\'s Index': youdens_index_balanced_orig_train_xgb,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_xgb,
    'Discriminant Power': discriminant_power_balanced_orig_train_xgb
}

testing_metrics['XGBoost'] = {
    'Accuracy': accuracy_test_balanced_orig_xgb,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_xgb,
    'Precision (Macro)': precision_test_balanced_orig_xgb,
    'Recall (Macro)': recall_test_balanced_orig_xgb,
    'F1 Score (Macro)': f1_test_balanced_orig_xgb,
    'AUC': auc_test_balanced_orig_xgb,
    'Cohen\'s Kappa': kappa_test_balanced_orig_xgb,
    'Type I Error': type1_error_balanced_orig_xgb,
    'Type II Error': type2_error_balanced_orig_xgb,
    'G-mean': g_mean_balanced_orig_xgb,
    'Youden\'s Index': youdens_index_balanced_orig_xgb,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_xgb,
    'Discriminant Power': discriminant_power_balanced_orig_xgb
}

# --- Collect Metrics for SVM ---
training_metrics['SVM'] = {
    'Accuracy': accuracy_train_balanced_orig_svm,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_svm,
    'Precision (Macro)': precision_train_balanced_orig_svm,
    'Recall (Macro)': recall_train_balanced_orig_svm,
    'F1 Score (Macro)': f1_train_balanced_orig_svm,
    'AUC': auc_train_balanced_orig_svm,
    'Cohen\'s Kappa': kappa_train_balanced_orig_svm,
    'Type I Error': type1_error_balanced_orig_train_svm,
    'Type II Error': type2_error_balanced_orig_train_svm,
    'G-mean': g_mean_balanced_orig_train_svm,
    'Youden\'s Index': youdens_index_balanced_orig_train_svm,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_svm,
    'Discriminant Power': discriminant_power_balanced_orig_train_svm
}

testing_metrics['SVM'] = {
    'Accuracy': accuracy_test_balanced_orig_svm,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_svm,
    'Precision (Macro)': precision_test_balanced_orig_svm,
    'Recall (Macro)': recall_test_balanced_orig_svm,
    'F1 Score (Macro)': f1_test_balanced_orig_svm,
    'AUC': auc_test_balanced_orig_svm,
    'Cohen\'s Kappa': kappa_test_balanced_orig_svm,
    'Type I Error': type1_error_balanced_orig_svm,
    'Type II Error': type2_error_balanced_orig_svm,
    'G-mean': g_mean_balanced_orig_svm,
    'Youden\'s Index': youdens_index_balanced_orig_svm,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_svm,
    'Discriminant Power': discriminant_power_balanced_orig_svm
}

# --- Collect Metrics for RF-MLP Hybrid ---
# Assuming the variables from the RF-MLP Hybrid evaluation are available
# Check if the variables exist before accessing them
if 'accuracy_train_hybrid' in globals():
    training_metrics['RF-MLP Hybrid'] = {
        'Accuracy': accuracy_train_hybrid,
        'Balanced Accuracy': balanced_accuracy_train_hybrid,
        'Precision (Macro)': precision_train_hybrid,
        'Recall (Macro)': recall_train_hybrid,
        'F1 Score (Macro)': f1_train_hybrid,
        'AUC': auc_train_hybrid,
        'Cohen\'s Kappa': kappa_train_hybrid,
        'Type I Error': type1_error_hybrid_train,
        'Type II Error': type2_error_hybrid_train,
        'G-mean': g_mean_hybrid_train,
        'Youden\'s Index': youdens_index_hybrid_train,
        'Negative Likelihood Ratio (NLiR)': nlir_hybrid_train,
        'Discriminant Power': discriminant_power_hybrid_train
    }
else:
    training_metrics['RF-MLP Hybrid'] = {metric: 'N/A' for metric in training_metrics['Logistic Regression'].keys()} # Fill with N/A if not run

if 'accuracy_test_hybrid' in globals():
    testing_metrics['RF-MLP Hybrid'] = {
        'Accuracy': accuracy_test_hybrid,
        'Balanced Accuracy': balanced_accuracy_test_hybrid,
        'Precision (Macro)': precision_test_hybrid,
        'Recall (Macro)': recall_test_hybrid,
        'F1 Score (Macro)': f1_test_hybrid,
        'AUC': auc_test_hybrid,
        'Cohen\'s Kappa': kappa_test_hybrid,
        'Type I Error': type1_error_hybrid_test,
        'Type II Error': type2_error_hybrid_test,
        'G-mean': g_mean_hybrid_test,
        'Youden\'s Index': youdens_index_hybrid_test,
        'Negative Likelihood Ratio (NLiR)': nlir_hybrid_test,
        'Discriminant Power': discriminant_power_hybrid_test
    }
else:
    testing_metrics['RF-MLP Hybrid'] = {metric: 'N/A' for metric in testing_metrics['Logistic Regression'].keys()} # Fill with N/A if not run


# --- Create DataFrames for Comparison ---
training_results_df = pd.DataFrame.from_dict(training_metrics, orient='index')
testing_results_df = pd.DataFrame.from_dict(testing_metrics, orient='index')

# Display the comparison tables
print("--- Model Performance Comparison (Training Data) ---")
display(training_results_df)

print("\n--- Model Performance Comparison (Testing Data) ---")
display(testing_results_df)

import pandas as pd

# Initialize dictionaries to store performance metrics for each model on training and testing data
training_metrics = {}
testing_metrics = {}

# --- Collect Metrics for Logistic Regression ---
training_metrics['Logistic Regression'] = {
    'Accuracy': accuracy_train_balanced_orig_lr * 100,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_lr * 100,
    'Precision (Macro)': precision_train_balanced_orig_lr * 100,
    'Recall (Macro)': recall_train_balanced_orig_lr * 100,
    'F1 Score (Macro)': f1_train_balanced_orig_lr * 100,
    'AUC': auc_train_balanced_orig_lr * 100,
    'Cohen\'s Kappa': kappa_train_balanced_orig_lr,
    'Type I Error': type1_error_balanced_orig_train_lr * 100,
    'Type II Error': type2_error_balanced_orig_train_lr * 100,
    'G-mean': g_mean_balanced_orig_train_lr * 100,
    'Youden\'s Index': youdens_index_balanced_orig_train_lr * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_lr,
    'Discriminant Power': discriminant_power_balanced_orig_train_lr
}

testing_metrics['Logistic Regression'] = {
    'Accuracy': accuracy_test_balanced_orig_lr * 100,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_lr * 100,
    'Precision (Macro)': precision_test_balanced_orig_lr * 100,
    'Recall (Macro)': recall_test_balanced_orig_lr * 100,
    'F1 Score (Macro)': f1_test_balanced_orig_lr * 100,
    'AUC': auc_test_balanced_orig_lr * 100,
    'Cohen\'s Kappa': kappa_test_balanced_orig_lr,
    'Type I Error': type1_error_balanced_orig_lr * 100,
    'Type II Error': type2_error_balanced_orig_lr * 100,
    'G-mean': g_mean_balanced_orig_lr * 100,
    'Youden\'s Index': youdens_index_balanced_orig_lr * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_lr,
    'Discriminant Power': discriminant_power_balanced_orig_lr
}

# --- Collect Metrics for Decision Tree ---
training_metrics['Decision Tree'] = {
    'Accuracy': accuracy_train_balanced_orig_dt * 100,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_dt * 100,
    'Precision (Macro)': precision_train_balanced_orig_dt * 100,
    'Recall (Macro)': recall_train_balanced_orig_dt * 100,
    'F1 Score (Macro)': f1_train_balanced_orig_dt * 100,
    'AUC': auc_train_balanced_orig_dt * 100,
    'Cohen\'s Kappa': kappa_train_balanced_orig_dt,
    'Type I Error': type1_error_balanced_orig_train_dt * 100,
    'Type II Error': type2_error_balanced_orig_train_dt * 100,
    'G-mean': g_mean_balanced_orig_train_dt * 100,
    'Youden\'s Index': youdens_index_balanced_orig_train_dt * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_dt,
    'Discriminant Power': discriminant_power_balanced_orig_train_dt
}

testing_metrics['Decision Tree'] = {
    'Accuracy': accuracy_test_balanced_orig_dt * 100,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_dt * 100,
    'Precision (Macro)': precision_test_balanced_orig_dt * 100,
    'Recall (Macro)': recall_test_balanced_orig_dt * 100,
    'F1 Score (Macro)': f1_test_balanced_orig_dt * 100,
    'AUC': auc_test_balanced_orig_dt * 100, # Assuming AUC was calculated for testing data
    'Cohen\'s Kappa': kappa_test_balanced_orig_dt,
    'Type I Error': type1_error_balanced_orig_dt * 100,
    'Type II Error': type2_error_balanced_orig_dt * 100,
    'G-mean': g_mean_balanced_orig_dt * 100,
    'Youden\'s Index': youdens_index_balanced_orig_dt * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_dt,
    'Discriminant Power': discriminant_power_balanced_orig_dt
}


# --- Collect Metrics for XGBoost ---
training_metrics['XGBoost'] = {
    'Accuracy': accuracy_train_balanced_orig_xgb * 100,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_xgb * 100,
    'Precision (Macro)': precision_train_balanced_orig_xgb * 100,
    'Recall (Macro)': recall_train_balanced_orig_xgb * 100,
    'F1 Score (Macro)': f1_train_balanced_orig_xgb * 100,
    'AUC': auc_train_balanced_orig_xgb * 100,
    'Cohen\'s Kappa': kappa_train_balanced_orig_xgb,
    'Type I Error': type1_error_balanced_orig_train_xgb * 100,
    'Type II Error': type2_error_balanced_orig_train_xgb * 100,
    'G-mean': g_mean_balanced_orig_train_xgb * 100,
    'Youden\'s Index': youdens_index_balanced_orig_train_xgb * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_xgb,
    'Discriminant Power': discriminant_power_balanced_orig_train_xgb
}

testing_metrics['XGBoost'] = {
    'Accuracy': accuracy_test_balanced_orig_xgb * 100,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_xgb * 100,
    'Precision (Macro)': precision_test_balanced_orig_xgb * 100,
    'Recall (Macro)': recall_test_balanced_orig_xgb * 100,
    'F1 Score (Macro)': f1_test_balanced_orig_xgb * 100,
    'AUC': auc_test_balanced_orig_xgb * 100,
    'Cohen\'s Kappa': kappa_test_balanced_orig_xgb,
    'Type I Error': type1_error_balanced_orig_xgb * 100,
    'Type II Error': type2_error_balanced_orig_xgb * 100,
    'G-mean': g_mean_balanced_orig_xgb * 100,
    'Youden\'s Index': youdens_index_balanced_orig_xgb * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_xgb,
    'Discriminant Power': discriminant_power_balanced_orig_xgb
}

# --- Collect Metrics for SVM ---
training_metrics['SVM'] = {
    'Accuracy': accuracy_train_balanced_orig_svm * 100,
    'Balanced Accuracy': balanced_accuracy_train_balanced_orig_svm * 100,
    'Precision (Macro)': precision_train_balanced_orig_svm * 100,
    'Recall (Macro)': recall_train_balanced_orig_svm * 100,
    'F1 Score (Macro)': f1_train_balanced_orig_svm * 100,
    'AUC': auc_train_balanced_orig_svm * 100,
    'Cohen\'s Kappa': kappa_train_balanced_orig_svm,
    'Type I Error': type1_error_balanced_orig_train_svm * 100,
    'Type II Error': type2_error_balanced_orig_train_svm * 100,
    'G-mean': g_mean_balanced_orig_train_svm * 100,
    'Youden\'s Index': youdens_index_balanced_orig_train_svm * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_train_svm,
    'Discriminant Power': discriminant_power_balanced_orig_train_svm
}

testing_metrics['SVM'] = {
    'Accuracy': accuracy_test_balanced_orig_svm * 100,
    'Balanced Accuracy': balanced_accuracy_test_balanced_orig_svm * 100,
    'Precision (Macro)': precision_test_balanced_orig_svm * 100,
    'Recall (Macro)': recall_test_balanced_orig_svm * 100,
    'F1 Score (Macro)': f1_test_balanced_orig_svm * 100,
    'AUC': auc_test_balanced_orig_svm * 100,
    'Cohen\'s Kappa': kappa_test_balanced_orig_svm,
    'Type I Error': type1_error_balanced_orig_svm * 100,
    'Type II Error': type2_error_balanced_orig_svm * 100,
    'G-mean': g_mean_balanced_orig_svm * 100,
    'Youden\'s Index': youdens_index_balanced_orig_svm * 100,
    'Negative Likelihood Ratio (NLiR)': nlir_balanced_orig_svm,
    'Discriminant Power': discriminant_power_balanced_orig_svm
}

# --- Collect Metrics for RF-MLP Hybrid ---
# Assuming the variables from the RF-MLP Hybrid evaluation are available
# Check if the variables exist before accessing them
if 'accuracy_train_hybrid' in globals():
    training_metrics['RF-MLP Hybrid'] = {
        'Accuracy': accuracy_train_hybrid * 100,
        'Balanced Accuracy': balanced_accuracy_train_hybrid * 100,
        'Precision (Macro)': precision_train_hybrid * 100,
        'Recall (Macro)': recall_train_hybrid * 100,
        'F1 Score (Macro)': f1_train_hybrid * 100,
        'AUC': auc_train_hybrid * 100,
        'Cohen\'s Kappa': kappa_train_hybrid,
        'Type I Error': type1_error_hybrid_train * 100,
        'Type II Error': type2_error_hybrid_train * 100,
        'G-mean': g_mean_hybrid_train * 100,
        'Youden\'s Index': youdens_index_hybrid_train * 100,
        'Negative Likelihood Ratio (NLiR)': nlir_hybrid_train,
        'Discriminant Power': discriminant_power_hybrid_train
    }
else:
     # Placeholder if the hybrid model wasn't run
    training_metrics['RF-MLP Hybrid'] = {
        'Accuracy': 'N/A', 'Balanced Accuracy': 'N/A', 'Precision (Macro)': 'N/A',
        'Recall (Macro)': 'N/A', 'F1 Score (Macro)': 'N/A', 'AUC': 'N/A',
        'Cohen\'s Kappa': 'N/A', 'Type I Error': 'N/A', 'Type II Error': 'N/A',
        'G-mean': 'N/A', 'Youden\'s Index': 'N/A', 'Negative Likelihood Ratio (NLiR)': 'N/A',
        'Discriminant Power': 'N/A'
    }

if 'accuracy_test_hybrid' in globals():
    testing_metrics['RF-MLP Hybrid'] = {
        'Accuracy': accuracy_test_hybrid * 100,
        'Balanced Accuracy': balanced_accuracy_test_hybrid * 100,
        'Precision (Macro)': precision_test_hybrid * 100,
        'Recall (Macro)': recall_test_hybrid * 100,
        'F1 Score (Macro)': f1_test_hybrid * 100,
        'AUC': auc_test_hybrid * 100,
        'Cohen\'s Kappa': kappa_test_hybrid,
        'Type I Error': type1_error_hybrid_test * 100,
        'Type II Error': type2_error_hybrid_test * 100,
        'G-mean': g_mean_hybrid_test * 100,
        'Youden\'s Index': youdens_index_hybrid_test * 100,
        'Negative Likelihood Ratio (NLiR)': nlir_hybrid_test,
        'Discriminant Power': discriminant_power_hybrid_test
    }
else:
    # Placeholder if the hybrid model wasn't run
    testing_metrics['RF-MLP Hybrid'] = {
        'Accuracy': 'N/A', 'Balanced Accuracy': 'N/A', 'Precision (Macro)': 'N/A',
        'Recall (Macro)': 'N/A', 'F1 Score (Macro)': 'N/A', 'AUC': 'N/A',
        'Cohen\'s Kappa': 'N/A', 'Type I Error': 'N/A', 'Type II Error': 'N/A',
        'G-mean': 'N/A', 'Youden\'s Index': 'N/A', 'Negative Likelihood Ratio (NLiR)': 'N/A',
        'Discriminant Power': 'N/A'
    }


# Define the desired order of models
model_order = ['Logistic Regression', 'Decision Tree', 'XGBoost', 'SVM', 'RF-MLP Hybrid']

# Create DataFrames for Comparison and reindex to set the desired order
training_results_df = pd.DataFrame.from_dict(training_metrics, orient='index').reindex(model_order)
testing_results_df = pd.DataFrame.from_dict(testing_metrics, orient='index').reindex(model_order)

# Display the comparison tables
print("--- Model Performance Comparison (Training Data) ---")
# Format percentage columns to two decimal places
training_results_df_formatted = training_results_df.copy()
percentage_cols = ['Accuracy', 'Balanced Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1 Score (Macro)', 'AUC', 'Type I Error', 'Type II Error', 'G-mean', 'Youden\'s Index']
for col in percentage_cols:
    if col in training_results_df_formatted.columns:
        # Handle non-numeric values (like 'N/A') before formatting
        training_results_df_formatted[col] = pd.to_numeric(training_results_df_formatted[col], errors='coerce').apply(lambda x: f'{x:.2f}%' if pd.notna(x) else 'N/A')

display(training_results_df_formatted)


print("\n--- Model Performance Comparison (Testing Data) ---")
# Format percentage columns to two decimal places
testing_results_df_formatted = testing_results_df.copy()
for col in percentage_cols:
     if col in testing_results_df_formatted.columns:
        # Handle non-numeric values (like 'N/A') before formatting
        testing_results_df_formatted[col] = pd.to_numeric(testing_results_df_formatted[col], errors='coerce').apply(lambda x: f'{x:.2f}%' if pd.notna(x) else 'N/A')

display(testing_results_df_formatted)

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=training_results_df_formatted)